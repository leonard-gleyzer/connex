{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning Connections\n",
    "\n",
    "In this example, we will train a DenseMLP, and then prune (remove) all connections whose weights are below some threshold in absolute value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1   Loss: 0.7124180793762207\n",
      "Epoch: 2   Loss: 0.471740186214447\n",
      "Epoch: 3   Loss: 0.5636301636695862\n",
      "Epoch: 4   Loss: 0.4581114947795868\n",
      "Epoch: 5   Loss: 0.34007692337036133\n",
      "Epoch: 6   Loss: 0.32127997279167175\n",
      "Epoch: 7   Loss: 0.3536826968193054\n",
      "Epoch: 8   Loss: 0.345887690782547\n",
      "Epoch: 9   Loss: 0.29597505927085876\n",
      "Epoch: 10   Loss: 0.2544102668762207\n",
      "Epoch: 11   Loss: 0.24971188604831696\n",
      "Epoch: 12   Loss: 0.26612362265586853\n",
      "Epoch: 13   Loss: 0.2702641189098358\n",
      "Epoch: 14   Loss: 0.24971722066402435\n",
      "Epoch: 15   Loss: 0.22087214887142181\n",
      "Epoch: 16   Loss: 0.20580922067165375\n",
      "Epoch: 17   Loss: 0.20780447125434875\n",
      "Epoch: 18   Loss: 0.21007828414440155\n",
      "Epoch: 19   Loss: 0.1984233856201172\n",
      "Epoch: 20   Loss: 0.17706890404224396\n",
      "Epoch: 21   Loss: 0.16016307473182678\n",
      "Epoch: 22   Loss: 0.15511168539524078\n",
      "Epoch: 23   Loss: 0.155961811542511\n",
      "Epoch: 24   Loss: 0.1516830176115036\n",
      "Epoch: 25   Loss: 0.13962800800800323\n",
      "Epoch: 26   Loss: 0.12779438495635986\n",
      "Epoch: 27   Loss: 0.12371464818716049\n",
      "Epoch: 28   Loss: 0.12478414177894592\n",
      "Epoch: 29   Loss: 0.12257537245750427\n",
      "Epoch: 30   Loss: 0.11461734771728516\n",
      "Epoch: 31   Loss: 0.10659317672252655\n",
      "Epoch: 32   Loss: 0.10368507355451584\n",
      "Epoch: 33   Loss: 0.10382530093193054\n",
      "Epoch: 34   Loss: 0.10142123699188232\n",
      "Epoch: 35   Loss: 0.09572411328554153\n",
      "Epoch: 36   Loss: 0.09128241240978241\n",
      "Epoch: 37   Loss: 0.09061063081026077\n",
      "Epoch: 38   Loss: 0.09075627475976944\n",
      "Epoch: 39   Loss: 0.0882754921913147\n",
      "Epoch: 40   Loss: 0.08425042778253555\n",
      "Epoch: 41   Loss: 0.08199948817491531\n",
      "Epoch: 42   Loss: 0.0816124752163887\n",
      "Epoch: 43   Loss: 0.08012596517801285\n",
      "Epoch: 44   Loss: 0.07683133333921432\n",
      "Epoch: 45   Loss: 0.07422269135713577\n",
      "Epoch: 46   Loss: 0.07343035936355591\n",
      "Epoch: 47   Loss: 0.0725143700838089\n",
      "Epoch: 48   Loss: 0.07014630734920502\n",
      "Epoch: 49   Loss: 0.06782779097557068\n",
      "Epoch: 50   Loss: 0.06676826626062393\n",
      "Epoch: 51   Loss: 0.06565256416797638\n",
      "Epoch: 52   Loss: 0.06341104954481125\n",
      "Epoch: 53   Loss: 0.06111516058444977\n",
      "Epoch: 54   Loss: 0.05970882624387741\n",
      "Epoch: 55   Loss: 0.05832800641655922\n",
      "Epoch: 56   Loss: 0.05623140186071396\n",
      "Epoch: 57   Loss: 0.05423720180988312\n",
      "Epoch: 58   Loss: 0.052895572036504745\n",
      "Epoch: 59   Loss: 0.051485348492860794\n",
      "Epoch: 60   Loss: 0.049574777483940125\n",
      "Epoch: 61   Loss: 0.04778434336185455\n",
      "Epoch: 62   Loss: 0.04639687389135361\n",
      "Epoch: 63   Loss: 0.04485248774290085\n",
      "Epoch: 64   Loss: 0.043042998760938644\n",
      "Epoch: 65   Loss: 0.04147044196724892\n",
      "Epoch: 66   Loss: 0.04015650972723961\n",
      "Epoch: 67   Loss: 0.03865491598844528\n",
      "Epoch: 68   Loss: 0.03705689311027527\n",
      "Epoch: 69   Loss: 0.03570118546485901\n",
      "Epoch: 70   Loss: 0.034429535269737244\n",
      "Epoch: 71   Loss: 0.03301023319363594\n",
      "Epoch: 72   Loss: 0.03168115392327309\n",
      "Epoch: 73   Loss: 0.030567506328225136\n",
      "Epoch: 74   Loss: 0.029436463490128517\n",
      "Epoch: 75   Loss: 0.028260067105293274\n",
      "Epoch: 76   Loss: 0.027235936373472214\n",
      "Epoch: 77   Loss: 0.026283547282218933\n",
      "Epoch: 78   Loss: 0.02527136355638504\n",
      "Epoch: 79   Loss: 0.02433624491095543\n",
      "Epoch: 80   Loss: 0.02353481948375702\n",
      "Epoch: 81   Loss: 0.022734276950359344\n",
      "Epoch: 82   Loss: 0.021960940212011337\n",
      "Epoch: 83   Loss: 0.021308785304427147\n",
      "Epoch: 84   Loss: 0.02069050632417202\n",
      "Epoch: 85   Loss: 0.02007497102022171\n",
      "Epoch: 86   Loss: 0.019552646204829216\n",
      "Epoch: 87   Loss: 0.019086623564362526\n",
      "Epoch: 88   Loss: 0.0186177846044302\n",
      "Epoch: 89   Loss: 0.018201328814029694\n",
      "Epoch: 90   Loss: 0.017826490104198456\n",
      "Epoch: 91   Loss: 0.01743679866194725\n",
      "Epoch: 92   Loss: 0.017071682959794998\n",
      "Epoch: 93   Loss: 0.016742179170250893\n",
      "Epoch: 94   Loss: 0.016402024775743484\n",
      "Epoch: 95   Loss: 0.016072329133749008\n",
      "Epoch: 96   Loss: 0.015765028074383736\n",
      "Epoch: 97   Loss: 0.015444771386682987\n",
      "Epoch: 98   Loss: 0.015128846280276775\n",
      "Epoch: 99   Loss: 0.014832018874585629\n",
      "Epoch: 100   Loss: 0.014527334831655025\n",
      "Epoch: 101   Loss: 0.01422534603625536\n",
      "Epoch: 102   Loss: 0.01393564511090517\n",
      "Epoch: 103   Loss: 0.013639116659760475\n",
      "Epoch: 104   Loss: 0.01334747951477766\n",
      "Epoch: 105   Loss: 0.01306784525513649\n",
      "Epoch: 106   Loss: 0.01278544683009386\n",
      "Epoch: 107   Loss: 0.012509547173976898\n",
      "Epoch: 108   Loss: 0.012242971919476986\n",
      "Epoch: 109   Loss: 0.011976576410233974\n",
      "Epoch: 110   Loss: 0.011720377951860428\n",
      "Epoch: 111   Loss: 0.01147368736565113\n",
      "Epoch: 112   Loss: 0.01122933067381382\n",
      "Epoch: 113   Loss: 0.010994543321430683\n",
      "Epoch: 114   Loss: 0.010765932500362396\n",
      "Epoch: 115   Loss: 0.010540798306465149\n",
      "Epoch: 116   Loss: 0.010324590839445591\n",
      "Epoch: 117   Loss: 0.010111949406564236\n",
      "Epoch: 118   Loss: 0.009902549907565117\n",
      "Epoch: 119   Loss: 0.009699417278170586\n",
      "Epoch: 120   Loss: 0.009498553350567818\n",
      "Epoch: 121   Loss: 0.00930237490683794\n",
      "Epoch: 122   Loss: 0.009110899642109871\n",
      "Epoch: 123   Loss: 0.008921072818338871\n",
      "Epoch: 124   Loss: 0.00873569492250681\n",
      "Epoch: 125   Loss: 0.008553522638976574\n",
      "Epoch: 126   Loss: 0.008375066332519054\n",
      "Epoch: 127   Loss: 0.00820456724613905\n",
      "Epoch: 128   Loss: 0.008048687130212784\n",
      "Epoch: 129   Loss: 0.00793672539293766\n",
      "Epoch: 130   Loss: 0.007957877591252327\n",
      "Epoch: 131   Loss: 0.008358325809240341\n",
      "Epoch: 132   Loss: 0.009606648236513138\n",
      "Epoch: 133   Loss: 0.011587896384298801\n",
      "Epoch: 134   Loss: 0.011754133738577366\n",
      "Epoch: 135   Loss: 0.008539949543774128\n",
      "Epoch: 136   Loss: 0.006798202637583017\n",
      "Epoch: 137   Loss: 0.008758152835071087\n",
      "Epoch: 138   Loss: 0.009230143390595913\n",
      "Epoch: 139   Loss: 0.006826704367995262\n",
      "Epoch: 140   Loss: 0.00662204110994935\n",
      "Epoch: 141   Loss: 0.008061617612838745\n",
      "Epoch: 142   Loss: 0.007017476484179497\n",
      "Epoch: 143   Loss: 0.0057989442721009254\n",
      "Epoch: 144   Loss: 0.00675214035436511\n",
      "Epoch: 145   Loss: 0.006742670200765133\n",
      "Epoch: 146   Loss: 0.005495416931807995\n",
      "Epoch: 147   Loss: 0.005743430927395821\n",
      "Epoch: 148   Loss: 0.006191667634993792\n",
      "Epoch: 149   Loss: 0.005323894787579775\n",
      "Epoch: 150   Loss: 0.005044146906584501\n",
      "Epoch: 151   Loss: 0.005529015325009823\n",
      "Epoch: 152   Loss: 0.005128428805619478\n",
      "Epoch: 153   Loss: 0.0045943958684802055\n",
      "Epoch: 154   Loss: 0.004866261500865221\n",
      "Epoch: 155   Loss: 0.004841616842895746\n",
      "Epoch: 156   Loss: 0.004319353029131889\n",
      "Epoch: 157   Loss: 0.004289999138563871\n",
      "Epoch: 158   Loss: 0.004444675520062447\n",
      "Epoch: 159   Loss: 0.004125180188566446\n",
      "Epoch: 160   Loss: 0.003863810794427991\n",
      "Epoch: 161   Loss: 0.00397136714309454\n",
      "Epoch: 162   Loss: 0.0039018699899315834\n",
      "Epoch: 163   Loss: 0.0036008497700095177\n",
      "Epoch: 164   Loss: 0.003522314131259918\n",
      "Epoch: 165   Loss: 0.0035735848359763622\n",
      "Epoch: 166   Loss: 0.00341805606149137\n",
      "Epoch: 167   Loss: 0.0032084921840578318\n",
      "Epoch: 168   Loss: 0.0031810856889933348\n",
      "Epoch: 169   Loss: 0.003174103330820799\n",
      "Epoch: 170   Loss: 0.003022704739123583\n",
      "Epoch: 171   Loss: 0.002871147822588682\n",
      "Epoch: 172   Loss: 0.0028396970592439175\n",
      "Epoch: 173   Loss: 0.002811489859595895\n",
      "Epoch: 174   Loss: 0.0026912824250757694\n",
      "Epoch: 175   Loss: 0.0025656824000179768\n",
      "Epoch: 176   Loss: 0.0025165725965052843\n",
      "Epoch: 177   Loss: 0.002486584708094597\n",
      "Epoch: 178   Loss: 0.002402606653049588\n",
      "Epoch: 179   Loss: 0.002294061705470085\n",
      "Epoch: 180   Loss: 0.002224402269348502\n",
      "Epoch: 181   Loss: 0.0021893971133977175\n",
      "Epoch: 182   Loss: 0.002139065880328417\n",
      "Epoch: 183   Loss: 0.002059503924101591\n",
      "Epoch: 184   Loss: 0.001976741710677743\n",
      "Epoch: 185   Loss: 0.0019234964856877923\n",
      "Epoch: 186   Loss: 0.0018863835139200091\n",
      "Epoch: 187   Loss: 0.0018393443897366524\n",
      "Epoch: 188   Loss: 0.0017753682332113385\n",
      "Epoch: 189   Loss: 0.001709521864540875\n",
      "Epoch: 190   Loss: 0.0016570371808484197\n",
      "Epoch: 191   Loss: 0.0016179849626496434\n",
      "Epoch: 192   Loss: 0.0015812893398106098\n",
      "Epoch: 193   Loss: 0.0015377984382212162\n",
      "Epoch: 194   Loss: 0.0014877531211823225\n",
      "Epoch: 195   Loss: 0.0014376261970028281\n",
      "Epoch: 196   Loss: 0.0013935138704255223\n",
      "Epoch: 197   Loss: 0.001356731285341084\n",
      "Epoch: 198   Loss: 0.0013244644505903125\n",
      "Epoch: 199   Loss: 0.0012929218355566263\n",
      "Epoch: 200   Loss: 0.0012596803717315197\n",
      "Epoch: 201   Loss: 0.0012246192200109363\n",
      "Epoch: 202   Loss: 0.0011889208108186722\n",
      "Epoch: 203   Loss: 0.0012069122167304158\n",
      "Epoch: 204   Loss: 0.0011283934582024813\n",
      "Epoch: 205   Loss: 0.0010984991677105427\n",
      "Epoch: 206   Loss: 0.0010725953616201878\n",
      "Epoch: 207   Loss: 0.0010445761727169156\n",
      "Epoch: 208   Loss: 0.0010199114913120866\n",
      "Epoch: 209   Loss: 0.0009944732300937176\n",
      "Epoch: 210   Loss: 0.000971490575466305\n",
      "Epoch: 211   Loss: 0.0009491420933045447\n",
      "Epoch: 212   Loss: 0.0009288907749578357\n",
      "Epoch: 213   Loss: 0.0009103944757953286\n",
      "Epoch: 214   Loss: 0.0008943004067987204\n",
      "Epoch: 215   Loss: 0.0008825139957480133\n",
      "Epoch: 216   Loss: 0.0008769791456870735\n",
      "Epoch: 217   Loss: 0.0008840184891596437\n",
      "Epoch: 218   Loss: 0.0009137787274084985\n",
      "Epoch: 219   Loss: 0.0009908685460686684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 220   Loss: 0.0011627969797700644\n",
      "Epoch: 221   Loss: 0.001527357380837202\n",
      "Epoch: 222   Loss: 0.002259867498651147\n",
      "Epoch: 223   Loss: 0.0035799986217170954\n",
      "Epoch: 224   Loss: 0.00548159796744585\n",
      "Epoch: 225   Loss: 0.006942305713891983\n",
      "Epoch: 226   Loss: 0.00605309521779418\n",
      "Epoch: 227   Loss: 0.0027579243760555983\n",
      "Epoch: 228   Loss: 0.0006995780277065933\n",
      "Epoch: 229   Loss: 0.00195307657122612\n",
      "Epoch: 230   Loss: 0.0037953979335725307\n",
      "Epoch: 231   Loss: 0.003040145616978407\n",
      "Epoch: 232   Loss: 0.0009854678064584732\n",
      "Epoch: 233   Loss: 0.0009928109357133508\n",
      "Epoch: 234   Loss: 0.002404278377071023\n",
      "Epoch: 235   Loss: 0.0022513431031256914\n",
      "Epoch: 236   Loss: 0.0008820308721624315\n",
      "Epoch: 237   Loss: 0.0008390072616748512\n",
      "Epoch: 238   Loss: 0.001778544276021421\n",
      "Epoch: 239   Loss: 0.0015943855978548527\n",
      "Epoch: 240   Loss: 0.0007007258245721459\n",
      "Epoch: 241   Loss: 0.000819189939647913\n",
      "Epoch: 242   Loss: 0.0014122298453003168\n",
      "Epoch: 243   Loss: 0.0011303203646093607\n",
      "Epoch: 244   Loss: 0.0005915021174587309\n",
      "Epoch: 245   Loss: 0.0008117115357890725\n",
      "Epoch: 246   Loss: 0.0011371803702786565\n",
      "Epoch: 247   Loss: 0.0008379684877581894\n",
      "Epoch: 248   Loss: 0.0005512114148586988\n",
      "Epoch: 249   Loss: 0.0007765218615531921\n",
      "Epoch: 250   Loss: 0.0009236892801709473\n",
      "Epoch: 251   Loss: 0.0006704549887217581\n",
      "Epoch: 252   Loss: 0.0005384802934713662\n",
      "Epoch: 253   Loss: 0.000718949711881578\n",
      "Epoch: 254   Loss: 0.0007678300607949495\n",
      "Epoch: 255   Loss: 0.0005811313749291003\n",
      "Epoch: 256   Loss: 0.0005268281674943864\n",
      "Epoch: 257   Loss: 0.0006538284360431135\n",
      "Epoch: 258   Loss: 0.0006620691274292767\n",
      "Epoch: 259   Loss: 0.000533316342625767\n",
      "Epoch: 260   Loss: 0.0005098262336105108\n",
      "Epoch: 261   Loss: 0.0005941509734839201\n",
      "Epoch: 262   Loss: 0.0005921223200857639\n",
      "Epoch: 263   Loss: 0.0005054001812823117\n",
      "Epoch: 264   Loss: 0.0004890990676358342\n",
      "Epoch: 265   Loss: 0.0005435235798358917\n",
      "Epoch: 266   Loss: 0.0005447134026326239\n",
      "Epoch: 267   Loss: 0.0004869099648203701\n",
      "Epoch: 268   Loss: 0.00046872126404196024\n",
      "Epoch: 269   Loss: 0.0005020778044126928\n",
      "Epoch: 270   Loss: 0.0005095285014249384\n",
      "Epoch: 271   Loss: 0.00047293660463765264\n",
      "Epoch: 272   Loss: 0.000451497093308717\n",
      "Epoch: 273   Loss: 0.0004683300503529608\n",
      "Epoch: 274   Loss: 0.0004801684117410332\n",
      "Epoch: 275   Loss: 0.0004606565344147384\n",
      "Epoch: 276   Loss: 0.00043876763083972037\n",
      "Epoch: 277   Loss: 0.0004417735617607832\n",
      "Epoch: 278   Loss: 0.00045328534906730056\n",
      "Epoch: 279   Loss: 0.0004474304150789976\n",
      "Epoch: 280   Loss: 0.0004298778367228806\n",
      "Epoch: 281   Loss: 0.0004227689641993493\n",
      "Epoch: 282   Loss: 0.00042870957986451685\n",
      "Epoch: 283   Loss: 0.0004310942895244807\n",
      "Epoch: 284   Loss: 0.00042187602957710624\n",
      "Epoch: 285   Loss: 0.00041103726834990084\n",
      "Epoch: 286   Loss: 0.0004089249123353511\n",
      "Epoch: 287   Loss: 0.0004120580560993403\n",
      "Epoch: 288   Loss: 0.0004107425338588655\n",
      "Epoch: 289   Loss: 0.0004032166616525501\n",
      "Epoch: 290   Loss: 0.00039620205643586814\n",
      "Epoch: 291   Loss: 0.0003944944473914802\n",
      "Epoch: 292   Loss: 0.0003953544073738158\n",
      "Epoch: 293   Loss: 0.00039351379382424057\n",
      "Epoch: 294   Loss: 0.0003883245517499745\n",
      "Epoch: 295   Loss: 0.00038271653465926647\n",
      "Epoch: 296   Loss: 0.0003803696308750659\n",
      "Epoch: 297   Loss: 0.0003799392143264413\n",
      "Epoch: 298   Loss: 0.0003784295404329896\n",
      "Epoch: 299   Loss: 0.0003746968286577612\n",
      "Epoch: 300   Loss: 0.0003702620742842555\n",
      "Epoch: 301   Loss: 0.0003671018057502806\n",
      "Epoch: 302   Loss: 0.00036551279481500387\n",
      "Epoch: 303   Loss: 0.000364190578693524\n",
      "Epoch: 304   Loss: 0.0003618772607296705\n",
      "Epoch: 305   Loss: 0.0003585633239708841\n",
      "Epoch: 306   Loss: 0.00035520255914889276\n",
      "Epoch: 307   Loss: 0.00035260923323221505\n",
      "Epoch: 308   Loss: 0.00035077729262411594\n",
      "Epoch: 309   Loss: 0.0003490637755021453\n",
      "Epoch: 310   Loss: 0.0003468950162641704\n",
      "Epoch: 311   Loss: 0.0003442071028985083\n",
      "Epoch: 312   Loss: 0.0003413857484702021\n",
      "Epoch: 313   Loss: 0.00033885135781019926\n",
      "Epoch: 314   Loss: 0.0003367320750840008\n",
      "Epoch: 315   Loss: 0.0003348502214066684\n",
      "Epoch: 316   Loss: 0.0003329186874907464\n",
      "Epoch: 317   Loss: 0.00033076261752285063\n",
      "Epoch: 318   Loss: 0.000328402646118775\n",
      "Epoch: 319   Loss: 0.00032599776750430465\n",
      "Epoch: 320   Loss: 0.0003237104101572186\n",
      "Epoch: 321   Loss: 0.0003216023033019155\n",
      "Epoch: 322   Loss: 0.0003196397447027266\n",
      "Epoch: 323   Loss: 0.00031772872898727655\n",
      "Epoch: 324   Loss: 0.0003157765604555607\n",
      "Epoch: 325   Loss: 0.0003137494786642492\n",
      "Epoch: 326   Loss: 0.0003116572625003755\n",
      "Epoch: 327   Loss: 0.0003095466236118227\n",
      "Epoch: 328   Loss: 0.00030746025731787086\n",
      "Epoch: 329   Loss: 0.0003054349508602172\n",
      "Epoch: 330   Loss: 0.00030347637948580086\n",
      "Epoch: 331   Loss: 0.00030157118453644216\n",
      "Epoch: 332   Loss: 0.0002995933755300939\n",
      "Epoch: 333   Loss: 0.0002978167321998626\n",
      "Epoch: 334   Loss: 0.0002958602271974087\n",
      "Epoch: 335   Loss: 0.0002939897822216153\n",
      "Epoch: 336   Loss: 0.0002920921251643449\n",
      "Epoch: 337   Loss: 0.00029024717514403164\n",
      "Epoch: 338   Loss: 0.0002884112764149904\n",
      "Epoch: 339   Loss: 0.00028658253722824156\n",
      "Epoch: 340   Loss: 0.00028479413595050573\n",
      "Epoch: 341   Loss: 0.00028298707911744714\n",
      "Epoch: 342   Loss: 0.0002812305756378919\n",
      "Epoch: 343   Loss: 0.0002794501488097012\n",
      "Epoch: 344   Loss: 0.0002777190238703042\n",
      "Epoch: 345   Loss: 0.0002759735216386616\n",
      "Epoch: 346   Loss: 0.00027426501037552953\n",
      "Epoch: 347   Loss: 0.000272569217486307\n",
      "Epoch: 348   Loss: 0.00027089955983683467\n",
      "Epoch: 349   Loss: 0.00026926968712359667\n",
      "Epoch: 350   Loss: 0.00026767505914904177\n",
      "Epoch: 351   Loss: 0.00026616748073138297\n",
      "Epoch: 352   Loss: 0.00026475550839677453\n",
      "Epoch: 353   Loss: 0.00026353204157203436\n",
      "Epoch: 354   Loss: 0.0002626000205054879\n",
      "Epoch: 355   Loss: 0.0002621911116875708\n",
      "Epoch: 356   Loss: 0.0002627017966005951\n",
      "Epoch: 357   Loss: 0.0002648883673828095\n",
      "Epoch: 358   Loss: 0.0002701845078263432\n",
      "Epoch: 359   Loss: 0.0002813436440192163\n",
      "Epoch: 360   Loss: 0.0003037565038539469\n",
      "Epoch: 361   Loss: 0.0003481174644548446\n",
      "Epoch: 362   Loss: 0.00043559769983403385\n",
      "Epoch: 363   Loss: 0.0006087575457058847\n",
      "Epoch: 364   Loss: 0.0009495871490798891\n",
      "Epoch: 365   Loss: 0.001613220083527267\n",
      "Epoch: 366   Loss: 0.0028422465547919273\n",
      "Epoch: 367   Loss: 0.004913087468594313\n",
      "Epoch: 368   Loss: 0.0076340576633811\n",
      "Epoch: 369   Loss: 0.009586235508322716\n",
      "Epoch: 370   Loss: 0.008104918524622917\n",
      "Epoch: 371   Loss: 0.003407940501347184\n",
      "Epoch: 372   Loss: 0.00028936422313563526\n",
      "Epoch: 373   Loss: 0.0018360100220888853\n",
      "Epoch: 374   Loss: 0.004538077395409346\n",
      "Epoch: 375   Loss: 0.003704454516991973\n",
      "Epoch: 376   Loss: 0.0008086562738753855\n",
      "Epoch: 377   Loss: 0.000626470020506531\n",
      "Epoch: 378   Loss: 0.0025839516893029213\n",
      "Epoch: 379   Loss: 0.0024569921661168337\n",
      "Epoch: 380   Loss: 0.0005715898587368429\n",
      "Epoch: 381   Loss: 0.0005636052810586989\n",
      "Epoch: 382   Loss: 0.0018454903038218617\n",
      "Epoch: 383   Loss: 0.001434300560504198\n",
      "Epoch: 384   Loss: 0.0002833400503732264\n",
      "Epoch: 385   Loss: 0.0006934741977602243\n",
      "Epoch: 386   Loss: 0.001365389907732606\n",
      "Epoch: 387   Loss: 0.0007164187263697386\n",
      "Epoch: 388   Loss: 0.0002295409794896841\n",
      "Epoch: 389   Loss: 0.0007860902696847916\n",
      "Epoch: 390   Loss: 0.0009074098779819906\n",
      "Epoch: 391   Loss: 0.00032606441527605057\n",
      "Epoch: 392   Loss: 0.00033603658084757626\n",
      "Epoch: 393   Loss: 0.0007284945459105074\n",
      "Epoch: 394   Loss: 0.0005191504023969173\n",
      "Epoch: 395   Loss: 0.00021250007557682693\n",
      "Epoch: 396   Loss: 0.00043398435809649527\n",
      "Epoch: 397   Loss: 0.0005556407268159091\n",
      "Epoch: 398   Loss: 0.0002890102914534509\n",
      "Epoch: 399   Loss: 0.0002432143228361383\n",
      "Epoch: 400   Loss: 0.0004390641115605831\n",
      "Epoch: 401   Loss: 0.0003753560013137758\n",
      "Epoch: 402   Loss: 0.00020824266539420933\n",
      "Epoch: 403   Loss: 0.00028960342751815915\n",
      "Epoch: 404   Loss: 0.00037374618113972247\n",
      "Epoch: 405   Loss: 0.00025784928584471345\n",
      "Epoch: 406   Loss: 0.00020658617722801864\n",
      "Epoch: 407   Loss: 0.00029893024475313723\n",
      "Epoch: 408   Loss: 0.000295645440928638\n",
      "Epoch: 409   Loss: 0.00020610295177903026\n",
      "Epoch: 410   Loss: 0.00022086514218244702\n",
      "Epoch: 411   Loss: 0.00027664031949825585\n",
      "Epoch: 412   Loss: 0.00023754026915412396\n",
      "Epoch: 413   Loss: 0.00019184414122719318\n",
      "Epoch: 414   Loss: 0.00022544701641891152\n",
      "Epoch: 415   Loss: 0.00024516775738447905\n",
      "Epoch: 416   Loss: 0.00020475925703067333\n",
      "Epoch: 417   Loss: 0.00019015905854757875\n",
      "Epoch: 418   Loss: 0.00021900977299083024\n",
      "Epoch: 419   Loss: 0.0002183110627811402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 420   Loss: 0.0001888366969069466\n",
      "Epoch: 421   Loss: 0.0001890507701318711\n",
      "Epoch: 422   Loss: 0.00020762355416081846\n",
      "Epoch: 423   Loss: 0.00019945208623539656\n",
      "Epoch: 424   Loss: 0.00018090875528287143\n",
      "Epoch: 425   Loss: 0.0001857195602497086\n",
      "Epoch: 426   Loss: 0.00019606210116762668\n",
      "Epoch: 427   Loss: 0.00018706954142544419\n",
      "Epoch: 428   Loss: 0.00017589307390153408\n",
      "Epoch: 429   Loss: 0.00018070625083055347\n",
      "Epoch: 430   Loss: 0.00018610002007335424\n",
      "Epoch: 431   Loss: 0.00017869115981739014\n",
      "Epoch: 432   Loss: 0.00017168254998978227\n",
      "Epoch: 433   Loss: 0.00017510591715108603\n",
      "Epoch: 434   Loss: 0.00017795473104342818\n",
      "Epoch: 435   Loss: 0.00017249849042855203\n",
      "Epoch: 436   Loss: 0.00016764766769483685\n",
      "Epoch: 437   Loss: 0.00016957851767074317\n",
      "Epoch: 438   Loss: 0.00017121086420957\n",
      "Epoch: 439   Loss: 0.00016743128071539104\n",
      "Epoch: 440   Loss: 0.00016370552475564182\n",
      "Epoch: 441   Loss: 0.00016439516912214458\n",
      "Epoch: 442   Loss: 0.0001654062361922115\n",
      "Epoch: 443   Loss: 0.0001629268517717719\n",
      "Epoch: 444   Loss: 0.00015989039093255997\n",
      "Epoch: 445   Loss: 0.00015963298210408539\n",
      "Epoch: 446   Loss: 0.00016056772437877953\n",
      "Epoch: 447   Loss: 0.00015867475303821266\n",
      "Epoch: 448   Loss: 0.00015620143676642329\n",
      "Epoch: 449   Loss: 0.00015528962831012905\n",
      "Epoch: 450   Loss: 0.00015543702465947717\n",
      "Epoch: 451   Loss: 0.00015455276297871023\n",
      "Epoch: 452   Loss: 0.00015262234956026077\n",
      "Epoch: 453   Loss: 0.00015130752581171691\n",
      "Epoch: 454   Loss: 0.00015099857409950346\n",
      "Epoch: 455   Loss: 0.0001504597021266818\n",
      "Epoch: 456   Loss: 0.0001490682188887149\n",
      "Epoch: 457   Loss: 0.00014763967192266136\n",
      "Epoch: 458   Loss: 0.00014689777162857354\n",
      "Epoch: 459   Loss: 0.0001464065135223791\n",
      "Epoch: 460   Loss: 0.00014545787416864187\n",
      "Epoch: 461   Loss: 0.00014416482008527964\n",
      "Epoch: 462   Loss: 0.0001431291166227311\n",
      "Epoch: 463   Loss: 0.00014247662329580635\n",
      "Epoch: 464   Loss: 0.0001417675957782194\n",
      "Epoch: 465   Loss: 0.00014074030332267284\n",
      "Epoch: 466   Loss: 0.00013963814126327634\n",
      "Epoch: 467   Loss: 0.00013876546290703118\n",
      "Epoch: 468   Loss: 0.0001380633475491777\n",
      "Epoch: 469   Loss: 0.0001372598489979282\n",
      "Epoch: 470   Loss: 0.00013628292072098702\n",
      "Epoch: 471   Loss: 0.00013530549767892808\n",
      "Epoch: 472   Loss: 0.000134479851112701\n",
      "Epoch: 473   Loss: 0.0001337353023700416\n",
      "Epoch: 474   Loss: 0.0001329173828708008\n",
      "Epoch: 475   Loss: 0.0001320061564911157\n",
      "Epoch: 476   Loss: 0.0001311001251451671\n",
      "Epoch: 477   Loss: 0.0001302847667830065\n",
      "Epoch: 478   Loss: 0.00012951977259945124\n",
      "Epoch: 479   Loss: 0.0001287195918848738\n",
      "Epoch: 480   Loss: 0.00012786460865754634\n",
      "Epoch: 481   Loss: 0.00012700616207439452\n",
      "Epoch: 482   Loss: 0.0001261946017621085\n",
      "Epoch: 483   Loss: 0.00012542400509119034\n",
      "Epoch: 484   Loss: 0.0001246454776264727\n",
      "Epoch: 485   Loss: 0.0001238356635440141\n",
      "Epoch: 486   Loss: 0.00012301464448682964\n",
      "Epoch: 487   Loss: 0.00012221508950460702\n",
      "Epoch: 488   Loss: 0.00012144404172431678\n",
      "Epoch: 489   Loss: 0.00012068329670000821\n",
      "Epoch: 490   Loss: 0.00011991094652330503\n",
      "Epoch: 491   Loss: 0.00011912686022697017\n",
      "Epoch: 492   Loss: 0.00011834430188173428\n",
      "Epoch: 493   Loss: 0.00011757958418456838\n",
      "Epoch: 494   Loss: 0.00011682998592732474\n",
      "Epoch: 495   Loss: 0.00011608518980210647\n",
      "Epoch: 496   Loss: 0.00011533277574926615\n",
      "Epoch: 497   Loss: 0.00011457913205958903\n",
      "Epoch: 498   Loss: 0.00011383048695279285\n",
      "Epoch: 499   Loss: 0.0001130900127463974\n",
      "Epoch: 500   Loss: 0.00011236095451749861\n"
     ]
    }
   ],
   "source": [
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import optax\n",
    "\n",
    "import connex as cnx\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "network = cnx.nn.DenseMLP(input_size=1, output_size=2, width=128, depth=4)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optim = optax.adam(1e-3)\n",
    "opt_state = optim.init(eqx.filter(network, eqx.is_array))\n",
    "\n",
    "\n",
    "# Define the loss function\n",
    "@eqx.filter_value_and_grad\n",
    "def loss_fn(model, x, y, keys):\n",
    "    preds = jax.vmap(model)(x, keys)\n",
    "    return jnp.mean((preds - y) ** 2)\n",
    "\n",
    "\n",
    "# Define a single training step\n",
    "@eqx.filter_jit\n",
    "def step(model, opt_state, x, y, keys):\n",
    "    loss, grads = loss_fn(model, x, y, keys)\n",
    "    updates, opt_state = optim.update(grads, opt_state, model)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, opt_state, loss\n",
    "\n",
    "\n",
    "# Toy data\n",
    "x = jnp.expand_dims(jnp.linspace(0, 2 * jnp.pi, 250), 1)\n",
    "y = jnp.hstack((jnp.cos(x), jnp.sin(x)))\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 500\n",
    "key = jr.PRNGKey(0)\n",
    "for epoch in range(n_epochs):\n",
    "    *keys, key = jr.split(key, x.shape[0] + 1)\n",
    "    keys = jnp.array(keys)\n",
    "    network, opt_state, loss = step(network, opt_state, x, y, keys)\n",
    "    print(f\"Epoch: {epoch + 1}   Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will prune the network of all connections whose weight is less than 0.01 in absolute value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the network to a NetworkX weighted DiGraph\n",
    "graph = network.to_networkx_weighted_digraph()\n",
    "\n",
    "# Set threshold and get all edges whose weight is below the threshold in absolute value\n",
    "threshold = 0.01\n",
    "edges = graph.edges(data=True)\n",
    "edges_below_threshold = [\n",
    "    (u, v) for u, v, data in edges if abs(data[\"weight\"]) < threshold\n",
    "]\n",
    "\n",
    "# Remove those connections from the network\n",
    "pruned_network = cnx.remove_connections(network, edges_below_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many connections are in the pruned network compared to the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in original network: 99842\n",
      "Number of edges in pruned network: 92059\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of edges in original network: {network._graph.number_of_edges()}\")\n",
    "print(f\"Number of edges in pruned network: {pruned_network._graph.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's see the loss for the pruned network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.00117882, dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = jnp.array(jr.split(key, x.shape[0]))\n",
    "loss_fn(pruned_network, x, y, keys)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "835bfa397ca6b52109e840e94dda5018b9451368d8fd0270017bd25fbadac04f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
