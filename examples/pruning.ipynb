{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning Connections\n",
    "\n",
    "In this example, we will train a DenseMLP, prune (remove) all connections whose weights are below some threshold in absolute value, and continue training with the pruned network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1   Loss: 0.7124180793762207\n",
      "Epoch: 2   Loss: 0.471740186214447\n",
      "Epoch: 3   Loss: 0.5636301636695862\n",
      "Epoch: 4   Loss: 0.4581114947795868\n",
      "Epoch: 5   Loss: 0.34007692337036133\n",
      "Epoch: 6   Loss: 0.32127997279167175\n",
      "Epoch: 7   Loss: 0.3536826968193054\n",
      "Epoch: 8   Loss: 0.345887690782547\n",
      "Epoch: 9   Loss: 0.29597505927085876\n",
      "Epoch: 10   Loss: 0.2544102668762207\n",
      "Epoch: 11   Loss: 0.24971188604831696\n",
      "Epoch: 12   Loss: 0.26612362265586853\n",
      "Epoch: 13   Loss: 0.2702641189098358\n",
      "Epoch: 14   Loss: 0.24971722066402435\n",
      "Epoch: 15   Loss: 0.22087214887142181\n",
      "Epoch: 16   Loss: 0.20580922067165375\n",
      "Epoch: 17   Loss: 0.20780447125434875\n",
      "Epoch: 18   Loss: 0.21007828414440155\n",
      "Epoch: 19   Loss: 0.1984233856201172\n",
      "Epoch: 20   Loss: 0.17706890404224396\n",
      "Epoch: 21   Loss: 0.16016307473182678\n",
      "Epoch: 22   Loss: 0.15511168539524078\n",
      "Epoch: 23   Loss: 0.155961811542511\n",
      "Epoch: 24   Loss: 0.1516830176115036\n",
      "Epoch: 25   Loss: 0.13962800800800323\n",
      "Epoch: 26   Loss: 0.12779438495635986\n",
      "Epoch: 27   Loss: 0.12371464818716049\n",
      "Epoch: 28   Loss: 0.12478414177894592\n",
      "Epoch: 29   Loss: 0.12257537245750427\n",
      "Epoch: 30   Loss: 0.11461734771728516\n",
      "Epoch: 31   Loss: 0.10659317672252655\n",
      "Epoch: 32   Loss: 0.10368507355451584\n",
      "Epoch: 33   Loss: 0.10382530093193054\n",
      "Epoch: 34   Loss: 0.10142123699188232\n",
      "Epoch: 35   Loss: 0.09572411328554153\n",
      "Epoch: 36   Loss: 0.09128241240978241\n",
      "Epoch: 37   Loss: 0.09061063081026077\n",
      "Epoch: 38   Loss: 0.09075627475976944\n",
      "Epoch: 39   Loss: 0.0882754921913147\n",
      "Epoch: 40   Loss: 0.08425042778253555\n",
      "Epoch: 41   Loss: 0.08199948817491531\n",
      "Epoch: 42   Loss: 0.0816124752163887\n",
      "Epoch: 43   Loss: 0.08012596517801285\n",
      "Epoch: 44   Loss: 0.07683133333921432\n",
      "Epoch: 45   Loss: 0.07422269135713577\n",
      "Epoch: 46   Loss: 0.07343035936355591\n",
      "Epoch: 47   Loss: 0.0725143700838089\n",
      "Epoch: 48   Loss: 0.07014630734920502\n",
      "Epoch: 49   Loss: 0.06782779097557068\n",
      "Epoch: 50   Loss: 0.06676826626062393\n",
      "Epoch: 51   Loss: 0.06565256416797638\n",
      "Epoch: 52   Loss: 0.06341104954481125\n",
      "Epoch: 53   Loss: 0.06111516058444977\n",
      "Epoch: 54   Loss: 0.05970882624387741\n",
      "Epoch: 55   Loss: 0.05832800641655922\n",
      "Epoch: 56   Loss: 0.05623140186071396\n",
      "Epoch: 57   Loss: 0.05423720180988312\n",
      "Epoch: 58   Loss: 0.052895572036504745\n",
      "Epoch: 59   Loss: 0.051485348492860794\n",
      "Epoch: 60   Loss: 0.049574777483940125\n",
      "Epoch: 61   Loss: 0.04778434336185455\n",
      "Epoch: 62   Loss: 0.04639687389135361\n",
      "Epoch: 63   Loss: 0.04485248774290085\n",
      "Epoch: 64   Loss: 0.043042998760938644\n",
      "Epoch: 65   Loss: 0.04147044196724892\n",
      "Epoch: 66   Loss: 0.04015650972723961\n",
      "Epoch: 67   Loss: 0.03865491598844528\n",
      "Epoch: 68   Loss: 0.03705689311027527\n",
      "Epoch: 69   Loss: 0.03570118546485901\n",
      "Epoch: 70   Loss: 0.034429535269737244\n",
      "Epoch: 71   Loss: 0.03301023319363594\n",
      "Epoch: 72   Loss: 0.03168115392327309\n",
      "Epoch: 73   Loss: 0.030567506328225136\n",
      "Epoch: 74   Loss: 0.029436463490128517\n",
      "Epoch: 75   Loss: 0.028260067105293274\n",
      "Epoch: 76   Loss: 0.027235936373472214\n",
      "Epoch: 77   Loss: 0.026283547282218933\n",
      "Epoch: 78   Loss: 0.02527136355638504\n",
      "Epoch: 79   Loss: 0.02433624491095543\n",
      "Epoch: 80   Loss: 0.02353481948375702\n",
      "Epoch: 81   Loss: 0.022734276950359344\n",
      "Epoch: 82   Loss: 0.021960940212011337\n",
      "Epoch: 83   Loss: 0.021308785304427147\n",
      "Epoch: 84   Loss: 0.02069050632417202\n",
      "Epoch: 85   Loss: 0.02007497102022171\n",
      "Epoch: 86   Loss: 0.019552646204829216\n",
      "Epoch: 87   Loss: 0.019086623564362526\n",
      "Epoch: 88   Loss: 0.0186177846044302\n",
      "Epoch: 89   Loss: 0.018201328814029694\n",
      "Epoch: 90   Loss: 0.017826490104198456\n",
      "Epoch: 91   Loss: 0.01743679866194725\n",
      "Epoch: 92   Loss: 0.017071682959794998\n",
      "Epoch: 93   Loss: 0.016742179170250893\n",
      "Epoch: 94   Loss: 0.016402024775743484\n",
      "Epoch: 95   Loss: 0.016072329133749008\n",
      "Epoch: 96   Loss: 0.015765028074383736\n",
      "Epoch: 97   Loss: 0.015444771386682987\n",
      "Epoch: 98   Loss: 0.015128846280276775\n",
      "Epoch: 99   Loss: 0.014832018874585629\n",
      "Epoch: 100   Loss: 0.014527334831655025\n",
      "Epoch: 101   Loss: 0.01422534603625536\n",
      "Epoch: 102   Loss: 0.01393564511090517\n",
      "Epoch: 103   Loss: 0.013639116659760475\n",
      "Epoch: 104   Loss: 0.01334747951477766\n",
      "Epoch: 105   Loss: 0.01306784525513649\n",
      "Epoch: 106   Loss: 0.01278544683009386\n",
      "Epoch: 107   Loss: 0.012509547173976898\n",
      "Epoch: 108   Loss: 0.012242971919476986\n",
      "Epoch: 109   Loss: 0.011976576410233974\n",
      "Epoch: 110   Loss: 0.011720377951860428\n",
      "Epoch: 111   Loss: 0.01147368736565113\n",
      "Epoch: 112   Loss: 0.01122933067381382\n",
      "Epoch: 113   Loss: 0.010994543321430683\n",
      "Epoch: 114   Loss: 0.010765932500362396\n",
      "Epoch: 115   Loss: 0.010540798306465149\n",
      "Epoch: 116   Loss: 0.010324590839445591\n",
      "Epoch: 117   Loss: 0.010111949406564236\n",
      "Epoch: 118   Loss: 0.009902549907565117\n",
      "Epoch: 119   Loss: 0.009699417278170586\n",
      "Epoch: 120   Loss: 0.009498553350567818\n",
      "Epoch: 121   Loss: 0.00930237490683794\n",
      "Epoch: 122   Loss: 0.009110899642109871\n",
      "Epoch: 123   Loss: 0.008921072818338871\n",
      "Epoch: 124   Loss: 0.00873569492250681\n",
      "Epoch: 125   Loss: 0.008553522638976574\n",
      "Epoch: 126   Loss: 0.008375066332519054\n",
      "Epoch: 127   Loss: 0.00820456724613905\n",
      "Epoch: 128   Loss: 0.008048687130212784\n",
      "Epoch: 129   Loss: 0.00793672539293766\n",
      "Epoch: 130   Loss: 0.007957877591252327\n",
      "Epoch: 131   Loss: 0.008358325809240341\n",
      "Epoch: 132   Loss: 0.009606648236513138\n",
      "Epoch: 133   Loss: 0.011587896384298801\n",
      "Epoch: 134   Loss: 0.011754133738577366\n",
      "Epoch: 135   Loss: 0.008539949543774128\n",
      "Epoch: 136   Loss: 0.006798202637583017\n",
      "Epoch: 137   Loss: 0.008758152835071087\n",
      "Epoch: 138   Loss: 0.009230143390595913\n",
      "Epoch: 139   Loss: 0.006826704367995262\n",
      "Epoch: 140   Loss: 0.00662204110994935\n",
      "Epoch: 141   Loss: 0.008061617612838745\n",
      "Epoch: 142   Loss: 0.007017476484179497\n",
      "Epoch: 143   Loss: 0.0057989442721009254\n",
      "Epoch: 144   Loss: 0.00675214035436511\n",
      "Epoch: 145   Loss: 0.006742670200765133\n",
      "Epoch: 146   Loss: 0.005495416931807995\n",
      "Epoch: 147   Loss: 0.005743430927395821\n",
      "Epoch: 148   Loss: 0.006191667634993792\n",
      "Epoch: 149   Loss: 0.005323894787579775\n",
      "Epoch: 150   Loss: 0.005044146906584501\n",
      "Epoch: 151   Loss: 0.005529015325009823\n",
      "Epoch: 152   Loss: 0.005128428805619478\n",
      "Epoch: 153   Loss: 0.0045943958684802055\n",
      "Epoch: 154   Loss: 0.004866261500865221\n",
      "Epoch: 155   Loss: 0.004841616842895746\n",
      "Epoch: 156   Loss: 0.004319353029131889\n",
      "Epoch: 157   Loss: 0.004289999138563871\n",
      "Epoch: 158   Loss: 0.004444675520062447\n",
      "Epoch: 159   Loss: 0.004125180188566446\n",
      "Epoch: 160   Loss: 0.003863810794427991\n",
      "Epoch: 161   Loss: 0.00397136714309454\n",
      "Epoch: 162   Loss: 0.0039018699899315834\n",
      "Epoch: 163   Loss: 0.0036008497700095177\n",
      "Epoch: 164   Loss: 0.003522314131259918\n",
      "Epoch: 165   Loss: 0.0035735848359763622\n",
      "Epoch: 166   Loss: 0.00341805606149137\n",
      "Epoch: 167   Loss: 0.0032084921840578318\n",
      "Epoch: 168   Loss: 0.0031810856889933348\n",
      "Epoch: 169   Loss: 0.003174103330820799\n",
      "Epoch: 170   Loss: 0.003022704739123583\n",
      "Epoch: 171   Loss: 0.002871147822588682\n",
      "Epoch: 172   Loss: 0.0028396970592439175\n",
      "Epoch: 173   Loss: 0.002811489859595895\n",
      "Epoch: 174   Loss: 0.0026912824250757694\n",
      "Epoch: 175   Loss: 0.0025656824000179768\n",
      "Epoch: 176   Loss: 0.0025165725965052843\n",
      "Epoch: 177   Loss: 0.002486584708094597\n",
      "Epoch: 178   Loss: 0.002402606653049588\n",
      "Epoch: 179   Loss: 0.002294061705470085\n",
      "Epoch: 180   Loss: 0.002224402269348502\n",
      "Epoch: 181   Loss: 0.0021893971133977175\n",
      "Epoch: 182   Loss: 0.002139065880328417\n",
      "Epoch: 183   Loss: 0.002059503924101591\n",
      "Epoch: 184   Loss: 0.001976741710677743\n",
      "Epoch: 185   Loss: 0.0019234964856877923\n",
      "Epoch: 186   Loss: 0.0018863835139200091\n",
      "Epoch: 187   Loss: 0.0018393443897366524\n",
      "Epoch: 188   Loss: 0.0017753682332113385\n",
      "Epoch: 189   Loss: 0.001709521864540875\n",
      "Epoch: 190   Loss: 0.0016570371808484197\n",
      "Epoch: 191   Loss: 0.0016179849626496434\n",
      "Epoch: 192   Loss: 0.0015812893398106098\n",
      "Epoch: 193   Loss: 0.0015377984382212162\n",
      "Epoch: 194   Loss: 0.0014877531211823225\n",
      "Epoch: 195   Loss: 0.0014376261970028281\n",
      "Epoch: 196   Loss: 0.0013935138704255223\n",
      "Epoch: 197   Loss: 0.001356731285341084\n",
      "Epoch: 198   Loss: 0.0013244644505903125\n",
      "Epoch: 199   Loss: 0.0012929218355566263\n",
      "Epoch: 200   Loss: 0.0012596803717315197\n",
      "Epoch: 201   Loss: 0.0012246192200109363\n",
      "Epoch: 202   Loss: 0.0011889208108186722\n",
      "Epoch: 203   Loss: 0.0012069122167304158\n",
      "Epoch: 204   Loss: 0.0011283934582024813\n",
      "Epoch: 205   Loss: 0.0010984991677105427\n",
      "Epoch: 206   Loss: 0.0010725953616201878\n",
      "Epoch: 207   Loss: 0.0010445761727169156\n",
      "Epoch: 208   Loss: 0.0010199114913120866\n",
      "Epoch: 209   Loss: 0.0009944732300937176\n",
      "Epoch: 210   Loss: 0.000971490575466305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 211   Loss: 0.0009491420933045447\n",
      "Epoch: 212   Loss: 0.0009288907749578357\n",
      "Epoch: 213   Loss: 0.0009103944757953286\n",
      "Epoch: 214   Loss: 0.0008943004067987204\n",
      "Epoch: 215   Loss: 0.0008825139957480133\n",
      "Epoch: 216   Loss: 0.0008769791456870735\n",
      "Epoch: 217   Loss: 0.0008840184891596437\n",
      "Epoch: 218   Loss: 0.0009137787274084985\n",
      "Epoch: 219   Loss: 0.0009908685460686684\n",
      "Epoch: 220   Loss: 0.0011627969797700644\n",
      "Epoch: 221   Loss: 0.001527357380837202\n",
      "Epoch: 222   Loss: 0.002259867498651147\n",
      "Epoch: 223   Loss: 0.0035799986217170954\n",
      "Epoch: 224   Loss: 0.00548159796744585\n",
      "Epoch: 225   Loss: 0.006942305713891983\n",
      "Epoch: 226   Loss: 0.00605309521779418\n",
      "Epoch: 227   Loss: 0.0027579243760555983\n",
      "Epoch: 228   Loss: 0.0006995780277065933\n",
      "Epoch: 229   Loss: 0.00195307657122612\n",
      "Epoch: 230   Loss: 0.0037953979335725307\n",
      "Epoch: 231   Loss: 0.003040145616978407\n",
      "Epoch: 232   Loss: 0.0009854678064584732\n",
      "Epoch: 233   Loss: 0.0009928109357133508\n",
      "Epoch: 234   Loss: 0.002404278377071023\n",
      "Epoch: 235   Loss: 0.0022513431031256914\n",
      "Epoch: 236   Loss: 0.0008820308721624315\n",
      "Epoch: 237   Loss: 0.0008390072616748512\n",
      "Epoch: 238   Loss: 0.001778544276021421\n",
      "Epoch: 239   Loss: 0.0015943855978548527\n",
      "Epoch: 240   Loss: 0.0007007258245721459\n",
      "Epoch: 241   Loss: 0.000819189939647913\n",
      "Epoch: 242   Loss: 0.0014122298453003168\n",
      "Epoch: 243   Loss: 0.0011303203646093607\n",
      "Epoch: 244   Loss: 0.0005915021174587309\n",
      "Epoch: 245   Loss: 0.0008117115357890725\n",
      "Epoch: 246   Loss: 0.0011371803702786565\n",
      "Epoch: 247   Loss: 0.0008379684877581894\n",
      "Epoch: 248   Loss: 0.0005512114148586988\n",
      "Epoch: 249   Loss: 0.0007765218615531921\n",
      "Epoch: 250   Loss: 0.0009236892801709473\n",
      "Epoch: 251   Loss: 0.0006704549887217581\n",
      "Epoch: 252   Loss: 0.0005384802934713662\n",
      "Epoch: 253   Loss: 0.000718949711881578\n",
      "Epoch: 254   Loss: 0.0007678300607949495\n",
      "Epoch: 255   Loss: 0.0005811313749291003\n",
      "Epoch: 256   Loss: 0.0005268281674943864\n",
      "Epoch: 257   Loss: 0.0006538284360431135\n",
      "Epoch: 258   Loss: 0.0006620691274292767\n",
      "Epoch: 259   Loss: 0.000533316342625767\n",
      "Epoch: 260   Loss: 0.0005098262336105108\n",
      "Epoch: 261   Loss: 0.0005941509734839201\n",
      "Epoch: 262   Loss: 0.0005921223200857639\n",
      "Epoch: 263   Loss: 0.0005054001812823117\n",
      "Epoch: 264   Loss: 0.0004890990676358342\n",
      "Epoch: 265   Loss: 0.0005435235798358917\n",
      "Epoch: 266   Loss: 0.0005447134026326239\n",
      "Epoch: 267   Loss: 0.0004869099648203701\n",
      "Epoch: 268   Loss: 0.00046872126404196024\n",
      "Epoch: 269   Loss: 0.0005020778044126928\n",
      "Epoch: 270   Loss: 0.0005095285014249384\n",
      "Epoch: 271   Loss: 0.00047293660463765264\n",
      "Epoch: 272   Loss: 0.000451497093308717\n",
      "Epoch: 273   Loss: 0.0004683300503529608\n",
      "Epoch: 274   Loss: 0.0004801684117410332\n",
      "Epoch: 275   Loss: 0.0004606565344147384\n",
      "Epoch: 276   Loss: 0.00043876763083972037\n",
      "Epoch: 277   Loss: 0.0004417735617607832\n",
      "Epoch: 278   Loss: 0.00045328534906730056\n",
      "Epoch: 279   Loss: 0.0004474304150789976\n",
      "Epoch: 280   Loss: 0.0004298778367228806\n",
      "Epoch: 281   Loss: 0.0004227689641993493\n",
      "Epoch: 282   Loss: 0.00042870957986451685\n",
      "Epoch: 283   Loss: 0.0004310942895244807\n",
      "Epoch: 284   Loss: 0.00042187602957710624\n",
      "Epoch: 285   Loss: 0.00041103726834990084\n",
      "Epoch: 286   Loss: 0.0004089249123353511\n",
      "Epoch: 287   Loss: 0.0004120580560993403\n",
      "Epoch: 288   Loss: 0.0004107425338588655\n",
      "Epoch: 289   Loss: 0.0004032166616525501\n",
      "Epoch: 290   Loss: 0.00039620205643586814\n",
      "Epoch: 291   Loss: 0.0003944944473914802\n",
      "Epoch: 292   Loss: 0.0003953544073738158\n",
      "Epoch: 293   Loss: 0.00039351379382424057\n",
      "Epoch: 294   Loss: 0.0003883245517499745\n",
      "Epoch: 295   Loss: 0.00038271653465926647\n",
      "Epoch: 296   Loss: 0.0003803696308750659\n",
      "Epoch: 297   Loss: 0.0003799392143264413\n",
      "Epoch: 298   Loss: 0.0003784295404329896\n",
      "Epoch: 299   Loss: 0.0003746968286577612\n",
      "Epoch: 300   Loss: 0.0003702620742842555\n",
      "Epoch: 301   Loss: 0.0003671018057502806\n",
      "Epoch: 302   Loss: 0.00036551279481500387\n",
      "Epoch: 303   Loss: 0.000364190578693524\n",
      "Epoch: 304   Loss: 0.0003618772607296705\n",
      "Epoch: 305   Loss: 0.0003585633239708841\n",
      "Epoch: 306   Loss: 0.00035520255914889276\n",
      "Epoch: 307   Loss: 0.00035260923323221505\n",
      "Epoch: 308   Loss: 0.00035077729262411594\n",
      "Epoch: 309   Loss: 0.0003490637755021453\n",
      "Epoch: 310   Loss: 0.0003468950162641704\n",
      "Epoch: 311   Loss: 0.0003442071028985083\n",
      "Epoch: 312   Loss: 0.0003413857484702021\n",
      "Epoch: 313   Loss: 0.00033885135781019926\n",
      "Epoch: 314   Loss: 0.0003367320750840008\n",
      "Epoch: 315   Loss: 0.0003348502214066684\n",
      "Epoch: 316   Loss: 0.0003329186874907464\n",
      "Epoch: 317   Loss: 0.00033076261752285063\n",
      "Epoch: 318   Loss: 0.000328402646118775\n",
      "Epoch: 319   Loss: 0.00032599776750430465\n",
      "Epoch: 320   Loss: 0.0003237104101572186\n",
      "Epoch: 321   Loss: 0.0003216023033019155\n",
      "Epoch: 322   Loss: 0.0003196397447027266\n",
      "Epoch: 323   Loss: 0.00031772872898727655\n",
      "Epoch: 324   Loss: 0.0003157765604555607\n",
      "Epoch: 325   Loss: 0.0003137494786642492\n",
      "Epoch: 326   Loss: 0.0003116572625003755\n",
      "Epoch: 327   Loss: 0.0003095466236118227\n",
      "Epoch: 328   Loss: 0.00030746025731787086\n",
      "Epoch: 329   Loss: 0.0003054349508602172\n",
      "Epoch: 330   Loss: 0.00030347637948580086\n",
      "Epoch: 331   Loss: 0.00030157118453644216\n",
      "Epoch: 332   Loss: 0.0002995933755300939\n",
      "Epoch: 333   Loss: 0.0002978167321998626\n",
      "Epoch: 334   Loss: 0.0002958602271974087\n",
      "Epoch: 335   Loss: 0.0002939897822216153\n",
      "Epoch: 336   Loss: 0.0002920921251643449\n",
      "Epoch: 337   Loss: 0.00029024717514403164\n",
      "Epoch: 338   Loss: 0.0002884112764149904\n",
      "Epoch: 339   Loss: 0.00028658253722824156\n",
      "Epoch: 340   Loss: 0.00028479413595050573\n",
      "Epoch: 341   Loss: 0.00028298707911744714\n",
      "Epoch: 342   Loss: 0.0002812305756378919\n",
      "Epoch: 343   Loss: 0.0002794501488097012\n",
      "Epoch: 344   Loss: 0.0002777190238703042\n",
      "Epoch: 345   Loss: 0.0002759735216386616\n",
      "Epoch: 346   Loss: 0.00027426501037552953\n",
      "Epoch: 347   Loss: 0.000272569217486307\n",
      "Epoch: 348   Loss: 0.00027089955983683467\n",
      "Epoch: 349   Loss: 0.00026926968712359667\n",
      "Epoch: 350   Loss: 0.00026767505914904177\n",
      "Epoch: 351   Loss: 0.00026616748073138297\n",
      "Epoch: 352   Loss: 0.00026475550839677453\n",
      "Epoch: 353   Loss: 0.00026353204157203436\n",
      "Epoch: 354   Loss: 0.0002626000205054879\n",
      "Epoch: 355   Loss: 0.0002621911116875708\n",
      "Epoch: 356   Loss: 0.0002627017966005951\n",
      "Epoch: 357   Loss: 0.0002648883673828095\n",
      "Epoch: 358   Loss: 0.0002701845078263432\n",
      "Epoch: 359   Loss: 0.0002813436440192163\n",
      "Epoch: 360   Loss: 0.0003037565038539469\n",
      "Epoch: 361   Loss: 0.0003481174644548446\n",
      "Epoch: 362   Loss: 0.00043559769983403385\n",
      "Epoch: 363   Loss: 0.0006087575457058847\n",
      "Epoch: 364   Loss: 0.0009495871490798891\n",
      "Epoch: 365   Loss: 0.001613220083527267\n",
      "Epoch: 366   Loss: 0.0028422465547919273\n",
      "Epoch: 367   Loss: 0.004913087468594313\n",
      "Epoch: 368   Loss: 0.0076340576633811\n",
      "Epoch: 369   Loss: 0.009586235508322716\n",
      "Epoch: 370   Loss: 0.008104918524622917\n",
      "Epoch: 371   Loss: 0.003407940501347184\n",
      "Epoch: 372   Loss: 0.00028936422313563526\n",
      "Epoch: 373   Loss: 0.0018360100220888853\n",
      "Epoch: 374   Loss: 0.004538077395409346\n",
      "Epoch: 375   Loss: 0.003704454516991973\n",
      "Epoch: 376   Loss: 0.0008086562738753855\n",
      "Epoch: 377   Loss: 0.000626470020506531\n",
      "Epoch: 378   Loss: 0.0025839516893029213\n",
      "Epoch: 379   Loss: 0.0024569921661168337\n",
      "Epoch: 380   Loss: 0.0005715898587368429\n",
      "Epoch: 381   Loss: 0.0005636052810586989\n",
      "Epoch: 382   Loss: 0.0018454903038218617\n",
      "Epoch: 383   Loss: 0.001434300560504198\n",
      "Epoch: 384   Loss: 0.0002833400503732264\n",
      "Epoch: 385   Loss: 0.0006934741977602243\n",
      "Epoch: 386   Loss: 0.001365389907732606\n",
      "Epoch: 387   Loss: 0.0007164187263697386\n",
      "Epoch: 388   Loss: 0.0002295409794896841\n",
      "Epoch: 389   Loss: 0.0007860902696847916\n",
      "Epoch: 390   Loss: 0.0009074098779819906\n",
      "Epoch: 391   Loss: 0.00032606441527605057\n",
      "Epoch: 392   Loss: 0.00033603658084757626\n",
      "Epoch: 393   Loss: 0.0007284945459105074\n",
      "Epoch: 394   Loss: 0.0005191504023969173\n",
      "Epoch: 395   Loss: 0.00021250007557682693\n",
      "Epoch: 396   Loss: 0.00043398435809649527\n",
      "Epoch: 397   Loss: 0.0005556407268159091\n",
      "Epoch: 398   Loss: 0.0002890102914534509\n",
      "Epoch: 399   Loss: 0.0002432143228361383\n",
      "Epoch: 400   Loss: 0.0004390641115605831\n",
      "Epoch: 401   Loss: 0.0003753560013137758\n",
      "Epoch: 402   Loss: 0.00020824266539420933\n",
      "Epoch: 403   Loss: 0.00028960342751815915\n",
      "Epoch: 404   Loss: 0.00037374618113972247\n",
      "Epoch: 405   Loss: 0.00025784928584471345\n",
      "Epoch: 406   Loss: 0.00020658617722801864\n",
      "Epoch: 407   Loss: 0.00029893024475313723\n",
      "Epoch: 408   Loss: 0.000295645440928638\n",
      "Epoch: 409   Loss: 0.00020610295177903026\n",
      "Epoch: 410   Loss: 0.00022086514218244702\n",
      "Epoch: 411   Loss: 0.00027664031949825585\n",
      "Epoch: 412   Loss: 0.00023754026915412396\n",
      "Epoch: 413   Loss: 0.00019184414122719318\n",
      "Epoch: 414   Loss: 0.00022544701641891152\n",
      "Epoch: 415   Loss: 0.00024516775738447905\n",
      "Epoch: 416   Loss: 0.00020475925703067333\n",
      "Epoch: 417   Loss: 0.00019015905854757875\n",
      "Epoch: 418   Loss: 0.00021900977299083024\n",
      "Epoch: 419   Loss: 0.0002183110627811402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 420   Loss: 0.0001888366969069466\n",
      "Epoch: 421   Loss: 0.0001890507701318711\n",
      "Epoch: 422   Loss: 0.00020762355416081846\n",
      "Epoch: 423   Loss: 0.00019945208623539656\n",
      "Epoch: 424   Loss: 0.00018090875528287143\n",
      "Epoch: 425   Loss: 0.0001857195602497086\n",
      "Epoch: 426   Loss: 0.00019606210116762668\n",
      "Epoch: 427   Loss: 0.00018706954142544419\n",
      "Epoch: 428   Loss: 0.00017589307390153408\n",
      "Epoch: 429   Loss: 0.00018070625083055347\n",
      "Epoch: 430   Loss: 0.00018610002007335424\n",
      "Epoch: 431   Loss: 0.00017869115981739014\n",
      "Epoch: 432   Loss: 0.00017168254998978227\n",
      "Epoch: 433   Loss: 0.00017510591715108603\n",
      "Epoch: 434   Loss: 0.00017795473104342818\n",
      "Epoch: 435   Loss: 0.00017249849042855203\n",
      "Epoch: 436   Loss: 0.00016764766769483685\n",
      "Epoch: 437   Loss: 0.00016957851767074317\n",
      "Epoch: 438   Loss: 0.00017121086420957\n",
      "Epoch: 439   Loss: 0.00016743128071539104\n",
      "Epoch: 440   Loss: 0.00016370552475564182\n",
      "Epoch: 441   Loss: 0.00016439516912214458\n",
      "Epoch: 442   Loss: 0.0001654062361922115\n",
      "Epoch: 443   Loss: 0.0001629268517717719\n",
      "Epoch: 444   Loss: 0.00015989039093255997\n",
      "Epoch: 445   Loss: 0.00015963298210408539\n",
      "Epoch: 446   Loss: 0.00016056772437877953\n",
      "Epoch: 447   Loss: 0.00015867475303821266\n",
      "Epoch: 448   Loss: 0.00015620143676642329\n",
      "Epoch: 449   Loss: 0.00015528962831012905\n",
      "Epoch: 450   Loss: 0.00015543702465947717\n",
      "Epoch: 451   Loss: 0.00015455276297871023\n",
      "Epoch: 452   Loss: 0.00015262234956026077\n",
      "Epoch: 453   Loss: 0.00015130752581171691\n",
      "Epoch: 454   Loss: 0.00015099857409950346\n",
      "Epoch: 455   Loss: 0.0001504597021266818\n",
      "Epoch: 456   Loss: 0.0001490682188887149\n",
      "Epoch: 457   Loss: 0.00014763967192266136\n",
      "Epoch: 458   Loss: 0.00014689777162857354\n",
      "Epoch: 459   Loss: 0.0001464065135223791\n",
      "Epoch: 460   Loss: 0.00014545787416864187\n",
      "Epoch: 461   Loss: 0.00014416482008527964\n",
      "Epoch: 462   Loss: 0.0001431291166227311\n",
      "Epoch: 463   Loss: 0.00014247662329580635\n",
      "Epoch: 464   Loss: 0.0001417675957782194\n",
      "Epoch: 465   Loss: 0.00014074030332267284\n",
      "Epoch: 466   Loss: 0.00013963814126327634\n",
      "Epoch: 467   Loss: 0.00013876546290703118\n",
      "Epoch: 468   Loss: 0.0001380633475491777\n",
      "Epoch: 469   Loss: 0.0001372598489979282\n",
      "Epoch: 470   Loss: 0.00013628292072098702\n",
      "Epoch: 471   Loss: 0.00013530549767892808\n",
      "Epoch: 472   Loss: 0.000134479851112701\n",
      "Epoch: 473   Loss: 0.0001337353023700416\n",
      "Epoch: 474   Loss: 0.0001329173828708008\n",
      "Epoch: 475   Loss: 0.0001320061564911157\n",
      "Epoch: 476   Loss: 0.0001311001251451671\n",
      "Epoch: 477   Loss: 0.0001302847667830065\n",
      "Epoch: 478   Loss: 0.00012951977259945124\n",
      "Epoch: 479   Loss: 0.0001287195918848738\n",
      "Epoch: 480   Loss: 0.00012786460865754634\n",
      "Epoch: 481   Loss: 0.00012700616207439452\n",
      "Epoch: 482   Loss: 0.0001261946017621085\n",
      "Epoch: 483   Loss: 0.00012542400509119034\n",
      "Epoch: 484   Loss: 0.0001246454776264727\n",
      "Epoch: 485   Loss: 0.0001238356635440141\n",
      "Epoch: 486   Loss: 0.00012301464448682964\n",
      "Epoch: 487   Loss: 0.00012221508950460702\n",
      "Epoch: 488   Loss: 0.00012144404172431678\n",
      "Epoch: 489   Loss: 0.00012068329670000821\n",
      "Epoch: 490   Loss: 0.00011991094652330503\n",
      "Epoch: 491   Loss: 0.00011912686022697017\n",
      "Epoch: 492   Loss: 0.00011834430188173428\n",
      "Epoch: 493   Loss: 0.00011757958418456838\n",
      "Epoch: 494   Loss: 0.00011682998592732474\n",
      "Epoch: 495   Loss: 0.00011608518980210647\n",
      "Epoch: 496   Loss: 0.00011533277574926615\n",
      "Epoch: 497   Loss: 0.00011457913205958903\n",
      "Epoch: 498   Loss: 0.00011383048695279285\n",
      "Epoch: 499   Loss: 0.0001130900127463974\n",
      "Epoch: 500   Loss: 0.00011236095451749861\n"
     ]
    }
   ],
   "source": [
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import optax\n",
    "\n",
    "import connex as cnx\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "network = cnx.nn.DenseMLP(input_size=1, output_size=2, width=128, depth=4)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optim = optax.adam(1e-3)\n",
    "opt_state = optim.init(eqx.filter(network, eqx.is_array))\n",
    "\n",
    "\n",
    "# Define the loss function\n",
    "@eqx.filter_value_and_grad\n",
    "def loss_fn(model, x, y, keys):\n",
    "    preds = jax.vmap(model)(x, keys)\n",
    "    return jnp.mean((preds - y) ** 2)\n",
    "\n",
    "\n",
    "# Define a single training step\n",
    "@eqx.filter_jit\n",
    "def step(model, opt_state, x, y, keys):\n",
    "    loss, grads = loss_fn(model, x, y, keys)\n",
    "    updates, opt_state = optim.update(grads, opt_state, model)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, opt_state, loss\n",
    "\n",
    "\n",
    "# Toy data\n",
    "x = jnp.expand_dims(jnp.linspace(0, 2 * jnp.pi, 250), 1)\n",
    "y = jnp.hstack((jnp.cos(x), jnp.sin(x)))\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 500\n",
    "key = jr.PRNGKey(0)\n",
    "for epoch in range(n_epochs):\n",
    "    *keys, key = jr.split(key, x.shape[0] + 1)\n",
    "    keys = jnp.array(keys)\n",
    "    network, opt_state, loss = step(network, opt_state, x, y, keys)\n",
    "    print(f\"Epoch: {epoch + 1}   Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will prune the network of all connections whose weight is less than 0.1 in absolute value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the network to a NetworkX weighted DiGraph\n",
    "graph = network.to_networkx_weighted_digraph()\n",
    "\n",
    "# Set threshold and get all edges whose weight is below the threshold in absolute value\n",
    "threshold = 0.1\n",
    "edges = graph.edges(data=True)\n",
    "edges_below_threshold = [\n",
    "    (u, v) for u, v, data in edges if abs(data[\"weight\"]) < threshold\n",
    "]\n",
    "\n",
    "# Remove those connections from the network\n",
    "pruned_network = cnx.remove_connections(network, edges_below_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many connections are in the pruned network compared to the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in original network: 99842\n",
      "Number of edges in pruned network: 32691\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of edges in original network: {network._graph.number_of_edges()}\")\n",
    "print(f\"Number of edges in pruned network: {pruned_network._graph.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's continue training with the pruned network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1   Loss: 0.44865846633911133\n",
      "Epoch: 2   Loss: 0.297453373670578\n",
      "Epoch: 3   Loss: 0.2075083702802658\n",
      "Epoch: 4   Loss: 0.17005939781665802\n",
      "Epoch: 5   Loss: 0.16803623735904694\n",
      "Epoch: 6   Loss: 0.178756982088089\n",
      "Epoch: 7   Loss: 0.18404240906238556\n",
      "Epoch: 8   Loss: 0.17665764689445496\n",
      "Epoch: 9   Loss: 0.1583200991153717\n",
      "Epoch: 10   Loss: 0.13481727242469788\n",
      "Epoch: 11   Loss: 0.11238483339548111\n",
      "Epoch: 12   Loss: 0.09569007158279419\n",
      "Epoch: 13   Loss: 0.08682015538215637\n",
      "Epoch: 14   Loss: 0.08502709120512009\n",
      "Epoch: 15   Loss: 0.08732927590608597\n",
      "Epoch: 16   Loss: 0.08992750197649002\n",
      "Epoch: 17   Loss: 0.08984372019767761\n",
      "Epoch: 18   Loss: 0.08592817932367325\n",
      "Epoch: 19   Loss: 0.07887118309736252\n",
      "Epoch: 20   Loss: 0.07050615549087524\n",
      "Epoch: 21   Loss: 0.06289120763540268\n",
      "Epoch: 22   Loss: 0.057517021894454956\n",
      "Epoch: 23   Loss: 0.05483124405145645\n",
      "Epoch: 24   Loss: 0.05418158322572708\n",
      "Epoch: 25   Loss: 0.05419432744383812\n",
      "Epoch: 26   Loss: 0.05344879627227783\n",
      "Epoch: 27   Loss: 0.05112697184085846\n",
      "Epoch: 28   Loss: 0.047300904989242554\n",
      "Epoch: 29   Loss: 0.04275308549404144\n",
      "Epoch: 30   Loss: 0.03850109875202179\n",
      "Epoch: 31   Loss: 0.03530504181981087\n",
      "Epoch: 32   Loss: 0.03337597846984863\n",
      "Epoch: 33   Loss: 0.03237534314393997\n",
      "Epoch: 34   Loss: 0.03165648505091667\n",
      "Epoch: 35   Loss: 0.030603433027863503\n",
      "Epoch: 36   Loss: 0.028904717415571213\n",
      "Epoch: 37   Loss: 0.026652328670024872\n",
      "Epoch: 38   Loss: 0.02424350008368492\n",
      "Epoch: 39   Loss: 0.02214827761054039\n",
      "Epoch: 40   Loss: 0.020663540810346603\n",
      "Epoch: 41   Loss: 0.019779011607170105\n",
      "Epoch: 42   Loss: 0.01921788416802883\n",
      "Epoch: 43   Loss: 0.01861315220594406\n",
      "Epoch: 44   Loss: 0.017705313861370087\n",
      "Epoch: 45   Loss: 0.01645323820412159\n",
      "Epoch: 46   Loss: 0.01501813717186451\n",
      "Epoch: 47   Loss: 0.013649988919496536\n",
      "Epoch: 48   Loss: 0.012543763034045696\n",
      "Epoch: 49   Loss: 0.01174100860953331\n",
      "Epoch: 50   Loss: 0.011128985323011875\n",
      "Epoch: 51   Loss: 0.01053199078887701\n",
      "Epoch: 52   Loss: 0.00982879102230072\n",
      "Epoch: 53   Loss: 0.009017590433359146\n",
      "Epoch: 54   Loss: 0.008196945302188396\n",
      "Epoch: 55   Loss: 0.007490157149732113\n",
      "Epoch: 56   Loss: 0.006966353394091129\n",
      "Epoch: 57   Loss: 0.006602393928915262\n",
      "Epoch: 58   Loss: 0.006303498055785894\n",
      "Epoch: 59   Loss: 0.005967745557427406\n",
      "Epoch: 60   Loss: 0.005550016649067402\n",
      "Epoch: 61   Loss: 0.00508141377940774\n",
      "Epoch: 62   Loss: 0.004635368473827839\n",
      "Epoch: 63   Loss: 0.00427208049222827\n",
      "Epoch: 64   Loss: 0.00400206446647644\n",
      "Epoch: 65   Loss: 0.0037876225542277098\n",
      "Epoch: 66   Loss: 0.0035747101064771414\n",
      "Epoch: 67   Loss: 0.003331399755552411\n",
      "Epoch: 68   Loss: 0.0030671374406665564\n",
      "Epoch: 69   Loss: 0.0028200086671859026\n",
      "Epoch: 70   Loss: 0.0026249231304973364\n",
      "Epoch: 71   Loss: 0.002489360049366951\n",
      "Epoch: 72   Loss: 0.0023930578026920557\n",
      "Epoch: 73   Loss: 0.0023065300192683935\n",
      "Epoch: 74   Loss: 0.002212058287113905\n",
      "Epoch: 75   Loss: 0.0021132801193743944\n",
      "Epoch: 76   Loss: 0.0020279199816286564\n",
      "Epoch: 77   Loss: 0.001970397774130106\n",
      "Epoch: 78   Loss: 0.0019393006805330515\n",
      "Epoch: 79   Loss: 0.0019196020439267159\n",
      "Epoch: 80   Loss: 0.0018952974351122975\n",
      "Epoch: 81   Loss: 0.001860613701865077\n",
      "Epoch: 82   Loss: 0.0018220278434455395\n",
      "Epoch: 83   Loss: 0.0017912297043949366\n",
      "Epoch: 84   Loss: 0.001774872769601643\n",
      "Epoch: 85   Loss: 0.001769428257830441\n",
      "Epoch: 86   Loss: 0.0017651250818744302\n",
      "Epoch: 87   Loss: 0.0017542503774166107\n",
      "Epoch: 88   Loss: 0.001736220670863986\n",
      "Epoch: 89   Loss: 0.0017161951400339603\n",
      "Epoch: 90   Loss: 0.001699726446531713\n",
      "Epoch: 91   Loss: 0.00168802950065583\n",
      "Epoch: 92   Loss: 0.0016775064868852496\n",
      "Epoch: 93   Loss: 0.0016635978827252984\n",
      "Epoch: 94   Loss: 0.0016449170652776957\n",
      "Epoch: 95   Loss: 0.0016238609096035361\n",
      "Epoch: 96   Loss: 0.0016039808979257941\n",
      "Epoch: 97   Loss: 0.001586979255080223\n",
      "Epoch: 98   Loss: 0.0015716471243649721\n",
      "Epoch: 99   Loss: 0.0015553984558209777\n",
      "Epoch: 100   Loss: 0.001536850817501545\n",
      "Epoch: 101   Loss: 0.0015168895479291677\n",
      "Epoch: 102   Loss: 0.001497505814768374\n",
      "Epoch: 103   Loss: 0.0014798549236729741\n",
      "Epoch: 104   Loss: 0.0014634234830737114\n",
      "Epoch: 105   Loss: 0.0014467129949480295\n",
      "Epoch: 106   Loss: 0.0014286875957623124\n",
      "Epoch: 107   Loss: 0.0014096926897764206\n",
      "Epoch: 108   Loss: 0.0013909534318372607\n",
      "Epoch: 109   Loss: 0.0013734039384871721\n",
      "Epoch: 110   Loss: 0.0013569992734119296\n",
      "Epoch: 111   Loss: 0.0013410071842372417\n",
      "Epoch: 112   Loss: 0.0013248017057776451\n",
      "Epoch: 113   Loss: 0.0013084354577586055\n",
      "Epoch: 114   Loss: 0.0012924633920192719\n",
      "Epoch: 115   Loss: 0.0012773174094036222\n",
      "Epoch: 116   Loss: 0.0012628877302631736\n",
      "Epoch: 117   Loss: 0.001248714281246066\n",
      "Epoch: 118   Loss: 0.001234440947882831\n",
      "Epoch: 119   Loss: 0.0012201076606288552\n",
      "Epoch: 120   Loss: 0.001206038985401392\n",
      "Epoch: 121   Loss: 0.0011924769496545196\n",
      "Epoch: 122   Loss: 0.0011793370358645916\n",
      "Epoch: 123   Loss: 0.001166364410892129\n",
      "Epoch: 124   Loss: 0.0011533923679962754\n",
      "Epoch: 125   Loss: 0.0011404793476685882\n",
      "Epoch: 126   Loss: 0.0011278340825811028\n",
      "Epoch: 127   Loss: 0.0011155653046444058\n",
      "Epoch: 128   Loss: 0.001103591755963862\n",
      "Epoch: 129   Loss: 0.0010917376494035125\n",
      "Epoch: 130   Loss: 0.001079906476661563\n",
      "Epoch: 131   Loss: 0.0010681470157578588\n",
      "Epoch: 132   Loss: 0.0010565560078248382\n",
      "Epoch: 133   Loss: 0.0010451844427734613\n",
      "Epoch: 134   Loss: 0.0010339758591726422\n",
      "Epoch: 135   Loss: 0.0010228455066680908\n",
      "Epoch: 136   Loss: 0.0010117669589817524\n",
      "Epoch: 137   Loss: 0.0010007944656535983\n",
      "Epoch: 138   Loss: 0.000989973428659141\n",
      "Epoch: 139   Loss: 0.0009793161880224943\n",
      "Epoch: 140   Loss: 0.0009687742567621171\n",
      "Epoch: 141   Loss: 0.0009583110804669559\n",
      "Epoch: 142   Loss: 0.0009479298023506999\n",
      "Epoch: 143   Loss: 0.0009376674424856901\n",
      "Epoch: 144   Loss: 0.000927553279325366\n",
      "Epoch: 145   Loss: 0.0009175770683214068\n",
      "Epoch: 146   Loss: 0.0009077119175344706\n",
      "Epoch: 147   Loss: 0.0008979440317489207\n",
      "Epoch: 148   Loss: 0.0008882887777872384\n",
      "Epoch: 149   Loss: 0.0008787673432379961\n",
      "Epoch: 150   Loss: 0.0008693879353813827\n",
      "Epoch: 151   Loss: 0.0008601348963566124\n",
      "Epoch: 152   Loss: 0.0008509953622706234\n",
      "Epoch: 153   Loss: 0.00084196642274037\n",
      "Epoch: 154   Loss: 0.0008330557029694319\n",
      "Epoch: 155   Loss: 0.0008242727490141988\n",
      "Epoch: 156   Loss: 0.0008156130788847804\n",
      "Epoch: 157   Loss: 0.0008070686599239707\n",
      "Epoch: 158   Loss: 0.0007986320997588336\n",
      "Epoch: 159   Loss: 0.0007903087534941733\n",
      "Epoch: 160   Loss: 0.0007821022882126272\n",
      "Epoch: 161   Loss: 0.0007740140426903963\n",
      "Epoch: 162   Loss: 0.0007660433184355497\n",
      "Epoch: 163   Loss: 0.0007581738173030317\n",
      "Epoch: 164   Loss: 0.0007504120003432035\n",
      "Epoch: 165   Loss: 0.0007427591481246054\n",
      "Epoch: 166   Loss: 0.0007352139218710363\n",
      "Epoch: 167   Loss: 0.0007277803961187601\n",
      "Epoch: 168   Loss: 0.000720444368198514\n",
      "Epoch: 169   Loss: 0.0007132123573683202\n",
      "Epoch: 170   Loss: 0.000706079532392323\n",
      "Epoch: 171   Loss: 0.0006990489782765508\n",
      "Epoch: 172   Loss: 0.0006921214517205954\n",
      "Epoch: 173   Loss: 0.0006852943915873766\n",
      "Epoch: 174   Loss: 0.0006785607547499239\n",
      "Epoch: 175   Loss: 0.0006719232187606394\n",
      "Epoch: 176   Loss: 0.000665382482111454\n",
      "Epoch: 177   Loss: 0.0006589420372620225\n",
      "Epoch: 178   Loss: 0.0006525889039039612\n",
      "Epoch: 179   Loss: 0.0006463294266723096\n",
      "Epoch: 180   Loss: 0.0006401602295227349\n",
      "Epoch: 181   Loss: 0.0006340841646306217\n",
      "Epoch: 182   Loss: 0.0006280980887822807\n",
      "Epoch: 183   Loss: 0.0006225079414434731\n",
      "Epoch: 184   Loss: 0.0006163895595818758\n",
      "Epoch: 185   Loss: 0.0006106652435846627\n",
      "Epoch: 186   Loss: 0.0006050258525647223\n",
      "Epoch: 187   Loss: 0.0005994709790684283\n",
      "Epoch: 188   Loss: 0.0005940005066804588\n",
      "Epoch: 189   Loss: 0.0005886112921871245\n",
      "Epoch: 190   Loss: 0.0005833043251186609\n",
      "Epoch: 191   Loss: 0.0005780755309388041\n",
      "Epoch: 192   Loss: 0.00057292974088341\n",
      "Epoch: 193   Loss: 0.0005678617162629962\n",
      "Epoch: 194   Loss: 0.0005628692451864481\n",
      "Epoch: 195   Loss: 0.0005579535500146449\n",
      "Epoch: 196   Loss: 0.0005531120114028454\n",
      "Epoch: 197   Loss: 0.0005483454442583025\n",
      "Epoch: 198   Loss: 0.0005436515784822404\n",
      "Epoch: 199   Loss: 0.0005390318692661822\n",
      "Epoch: 200   Loss: 0.0005344805540516973\n",
      "Epoch: 201   Loss: 0.0005299995536915958\n",
      "Epoch: 202   Loss: 0.0005255882861092687\n",
      "Epoch: 203   Loss: 0.0006074091652408242\n",
      "Epoch: 204   Loss: 0.0005181007436476648\n",
      "Epoch: 205   Loss: 0.0005134528619237244\n",
      "Epoch: 206   Loss: 0.0005094773950986564\n",
      "Epoch: 207   Loss: 0.000505542557220906\n",
      "Epoch: 208   Loss: 0.000500651600304991\n",
      "Epoch: 209   Loss: 0.0004970816662535071\n",
      "Epoch: 210   Loss: 0.0004932798910886049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 211   Loss: 0.0004889486590400338\n",
      "Epoch: 212   Loss: 0.0004856523300986737\n",
      "Epoch: 213   Loss: 0.00048154551768675447\n",
      "Epoch: 214   Loss: 0.0004779237788170576\n",
      "Epoch: 215   Loss: 0.0004745451151393354\n",
      "Epoch: 216   Loss: 0.00047061595250852406\n",
      "Epoch: 217   Loss: 0.00046736691729165614\n",
      "Epoch: 218   Loss: 0.000463794480310753\n",
      "Epoch: 219   Loss: 0.00046033455873839557\n",
      "Epoch: 220   Loss: 0.00045719873742200434\n",
      "Epoch: 221   Loss: 0.00045368174323812127\n",
      "Epoch: 222   Loss: 0.0004505550896283239\n",
      "Epoch: 223   Loss: 0.00044734077528119087\n",
      "Epoch: 224   Loss: 0.0004441167984623462\n",
      "Epoch: 225   Loss: 0.00044116273056715727\n",
      "Epoch: 226   Loss: 0.00043799146078526974\n",
      "Epoch: 227   Loss: 0.00043504146742634475\n",
      "Epoch: 228   Loss: 0.00043209639261476696\n",
      "Epoch: 229   Loss: 0.0004291234945412725\n",
      "Epoch: 230   Loss: 0.00042633965495042503\n",
      "Epoch: 231   Loss: 0.00042344536632299423\n",
      "Epoch: 232   Loss: 0.000420688244048506\n",
      "Epoch: 233   Loss: 0.0004179648240096867\n",
      "Epoch: 234   Loss: 0.00041522574611008167\n",
      "Epoch: 235   Loss: 0.0004126180137973279\n",
      "Epoch: 236   Loss: 0.00040995600284077227\n",
      "Epoch: 237   Loss: 0.00040738668758422136\n",
      "Epoch: 238   Loss: 0.0004048541886731982\n",
      "Epoch: 239   Loss: 0.000402325124014169\n",
      "Epoch: 240   Loss: 0.0003998850879725069\n",
      "Epoch: 241   Loss: 0.0003974256687797606\n",
      "Epoch: 242   Loss: 0.0003950368263758719\n",
      "Epoch: 243   Loss: 0.00039267478859983385\n",
      "Epoch: 244   Loss: 0.0003903317847289145\n",
      "Epoch: 245   Loss: 0.00038805135409347713\n",
      "Epoch: 246   Loss: 0.00038576938095502555\n",
      "Epoch: 247   Loss: 0.000383545906515792\n",
      "Epoch: 248   Loss: 0.00038134309579618275\n",
      "Epoch: 249   Loss: 0.0003791640338022262\n",
      "Epoch: 250   Loss: 0.00037703063571825624\n",
      "Epoch: 251   Loss: 0.00037490762770175934\n",
      "Epoch: 252   Loss: 0.0003728324663825333\n",
      "Epoch: 253   Loss: 0.00037077386514283717\n",
      "Epoch: 254   Loss: 0.0003687447460833937\n",
      "Epoch: 255   Loss: 0.000366748048691079\n",
      "Epoch: 256   Loss: 0.0003647658450063318\n",
      "Epoch: 257   Loss: 0.00036282293149270117\n",
      "Epoch: 258   Loss: 0.00036089736386202276\n",
      "Epoch: 259   Loss: 0.0003589992702472955\n",
      "Epoch: 260   Loss: 0.00035712678800337017\n",
      "Epoch: 261   Loss: 0.0003552737180143595\n",
      "Epoch: 262   Loss: 0.0003534519928507507\n",
      "Epoch: 263   Loss: 0.00035164548899047077\n",
      "Epoch: 264   Loss: 0.0003498667210806161\n",
      "Epoch: 265   Loss: 0.0003481073654256761\n",
      "Epoch: 266   Loss: 0.00034637004137039185\n",
      "Epoch: 267   Loss: 0.0003446546324994415\n",
      "Epoch: 268   Loss: 0.00034295787918381393\n",
      "Epoch: 269   Loss: 0.000341284234309569\n",
      "Epoch: 270   Loss: 0.0003396290121600032\n",
      "Epoch: 271   Loss: 0.000337994220899418\n",
      "Epoch: 272   Loss: 0.00033637863816693425\n",
      "Epoch: 273   Loss: 0.0003347800229676068\n",
      "Epoch: 274   Loss: 0.0003332012565806508\n",
      "Epoch: 275   Loss: 0.00033163800253532827\n",
      "Epoch: 276   Loss: 0.0003300960233900696\n",
      "Epoch: 277   Loss: 0.00032856903271749616\n",
      "Epoch: 278   Loss: 0.0003270584566053003\n",
      "Epoch: 279   Loss: 0.00032556563382968307\n",
      "Epoch: 280   Loss: 0.0003240887017454952\n",
      "Epoch: 281   Loss: 0.00032262736931443214\n",
      "Epoch: 282   Loss: 0.0003211823641322553\n",
      "Epoch: 283   Loss: 0.00031975150341168046\n",
      "Epoch: 284   Loss: 0.00031833621324039996\n",
      "Epoch: 285   Loss: 0.00031693559139966965\n",
      "Epoch: 286   Loss: 0.00031554981251247227\n",
      "Epoch: 287   Loss: 0.0003141755296383053\n",
      "Epoch: 288   Loss: 0.00031281576957553625\n",
      "Epoch: 289   Loss: 0.0003114720748271793\n",
      "Epoch: 290   Loss: 0.0003101396141573787\n",
      "Epoch: 291   Loss: 0.00030882147257216275\n",
      "Epoch: 292   Loss: 0.00030751392478123307\n",
      "Epoch: 293   Loss: 0.0003062189498450607\n",
      "Epoch: 294   Loss: 0.0003082396578975022\n",
      "Epoch: 295   Loss: 0.0003036712878383696\n",
      "Epoch: 296   Loss: 0.00030241295462474227\n",
      "Epoch: 297   Loss: 0.000301166350254789\n",
      "Epoch: 298   Loss: 0.00029992673080414534\n",
      "Epoch: 299   Loss: 0.0002986994222737849\n",
      "Epoch: 300   Loss: 0.0002974880044348538\n",
      "Epoch: 301   Loss: 0.0002962827275041491\n",
      "Epoch: 302   Loss: 0.00029508923762477934\n",
      "Epoch: 303   Loss: 0.0002939043042715639\n",
      "Epoch: 304   Loss: 0.00029273267136886716\n",
      "Epoch: 305   Loss: 0.0002915681980084628\n",
      "Epoch: 306   Loss: 0.00029041338711977005\n",
      "Epoch: 307   Loss: 0.00028926823870278895\n",
      "Epoch: 308   Loss: 0.00028813432436436415\n",
      "Epoch: 309   Loss: 0.00028700800612568855\n",
      "Epoch: 310   Loss: 0.0002858915540855378\n",
      "Epoch: 311   Loss: 0.0002847825817298144\n",
      "Epoch: 312   Loss: 0.00028368260245770216\n",
      "Epoch: 313   Loss: 0.00028259115060791373\n",
      "Epoch: 314   Loss: 0.0002815077605191618\n",
      "Epoch: 315   Loss: 0.0002804328396450728\n",
      "Epoch: 316   Loss: 0.0002793657185975462\n",
      "Epoch: 317   Loss: 0.00027830604813061655\n",
      "Epoch: 318   Loss: 0.00027725534164346755\n",
      "Epoch: 319   Loss: 0.00027621121262200177\n",
      "Epoch: 320   Loss: 0.000275175814749673\n",
      "Epoch: 321   Loss: 0.0002741458301898092\n",
      "Epoch: 322   Loss: 0.00027312475140206516\n",
      "Epoch: 323   Loss: 0.00027211025008000433\n",
      "Epoch: 324   Loss: 0.00027110212249681354\n",
      "Epoch: 325   Loss: 0.00027010159101337194\n",
      "Epoch: 326   Loss: 0.00026910746237263083\n",
      "Epoch: 327   Loss: 0.00026811964926309884\n",
      "Epoch: 328   Loss: 0.000267139810603112\n",
      "Epoch: 329   Loss: 0.00026616454124450684\n",
      "Epoch: 330   Loss: 0.0002651969844009727\n",
      "Epoch: 331   Loss: 0.00026423545205034316\n",
      "Epoch: 332   Loss: 0.0002640985476318747\n",
      "Epoch: 333   Loss: 0.00026234827237203717\n",
      "Epoch: 334   Loss: 0.0002613932592794299\n",
      "Epoch: 335   Loss: 0.00026046409038826823\n",
      "Epoch: 336   Loss: 0.0002595219702925533\n",
      "Epoch: 337   Loss: 0.0002586005721241236\n",
      "Epoch: 338   Loss: 0.00025767917395569384\n",
      "Epoch: 339   Loss: 0.00025676190853118896\n",
      "Epoch: 340   Loss: 0.00025585360708646476\n",
      "Epoch: 341   Loss: 0.000254946353379637\n",
      "Epoch: 342   Loss: 0.00025405222550034523\n",
      "Epoch: 343   Loss: 0.00025315317907370627\n",
      "Epoch: 344   Loss: 0.00025226856814697385\n",
      "Epoch: 345   Loss: 0.0002513798826839775\n",
      "Epoch: 346   Loss: 0.00025050641852431\n",
      "Epoch: 347   Loss: 0.0002496293745934963\n",
      "Epoch: 348   Loss: 0.00024876391398720443\n",
      "Epoch: 349   Loss: 0.00024789749295450747\n",
      "Epoch: 350   Loss: 0.0002470396284479648\n",
      "Epoch: 351   Loss: 0.000246185198193416\n",
      "Epoch: 352   Loss: 0.0002453348715789616\n",
      "Epoch: 353   Loss: 0.00024449158809147775\n",
      "Epoch: 354   Loss: 0.00024365037097595632\n",
      "Epoch: 355   Loss: 0.00024281661899294704\n",
      "Epoch: 356   Loss: 0.00024198276514653116\n",
      "Epoch: 357   Loss: 0.00024115775886457413\n",
      "Epoch: 358   Loss: 0.00024033410591073334\n",
      "Epoch: 359   Loss: 0.00023951679759193212\n",
      "Epoch: 360   Loss: 0.0002387028798693791\n",
      "Epoch: 361   Loss: 0.0002378940989729017\n",
      "Epoch: 362   Loss: 0.00023708846129011363\n",
      "Epoch: 363   Loss: 0.00023628809140063822\n",
      "Epoch: 364   Loss: 0.0002354908356210217\n",
      "Epoch: 365   Loss: 0.00023469705774914473\n",
      "Epoch: 366   Loss: 0.00023390854767058045\n",
      "Epoch: 367   Loss: 0.00023312347184401006\n",
      "Epoch: 368   Loss: 0.00023234273248817772\n",
      "Epoch: 369   Loss: 0.00023156547104008496\n",
      "Epoch: 370   Loss: 0.00023079306993167847\n",
      "Epoch: 371   Loss: 0.00023002368106972426\n",
      "Epoch: 372   Loss: 0.00022925782832317054\n",
      "Epoch: 373   Loss: 0.00022849664674140513\n",
      "Epoch: 374   Loss: 0.00022773744422011077\n",
      "Epoch: 375   Loss: 0.0002269838296342641\n",
      "Epoch: 376   Loss: 0.00022623328550253063\n",
      "Epoch: 377   Loss: 0.00022548688866663724\n",
      "Epoch: 378   Loss: 0.00022474383877124637\n",
      "Epoch: 379   Loss: 0.00022400301531888545\n",
      "Epoch: 380   Loss: 0.00022326801263261586\n",
      "Epoch: 381   Loss: 0.0002225357893621549\n",
      "Epoch: 382   Loss: 0.0002218069275841117\n",
      "Epoch: 383   Loss: 0.00022117342450655997\n",
      "Epoch: 384   Loss: 0.00022035898291505873\n",
      "Epoch: 385   Loss: 0.0002196404675487429\n",
      "Epoch: 386   Loss: 0.0002189245424233377\n",
      "Epoch: 387   Loss: 0.00021821279369760305\n",
      "Epoch: 388   Loss: 0.00021750382438767701\n",
      "Epoch: 389   Loss: 0.00021679853671230376\n",
      "Epoch: 390   Loss: 0.00021609691611956805\n",
      "Epoch: 391   Loss: 0.0002153986133635044\n",
      "Epoch: 392   Loss: 0.00021470240608323365\n",
      "Epoch: 393   Loss: 0.00021401053527370095\n",
      "Epoch: 394   Loss: 0.00021332077449187636\n",
      "Epoch: 395   Loss: 0.00021263558301143348\n",
      "Epoch: 396   Loss: 0.00021195328736212105\n",
      "Epoch: 397   Loss: 0.00021127337822690606\n",
      "Epoch: 398   Loss: 0.0002105964522343129\n",
      "Epoch: 399   Loss: 0.00020992301870137453\n",
      "Epoch: 400   Loss: 0.00020925332501064986\n",
      "Epoch: 401   Loss: 0.0002085854794131592\n",
      "Epoch: 402   Loss: 0.00020792205759789795\n",
      "Epoch: 403   Loss: 0.00020725974172819406\n",
      "Epoch: 404   Loss: 0.00020660037989728153\n",
      "Epoch: 405   Loss: 0.00020594535453710705\n",
      "Epoch: 406   Loss: 0.00020529371977318078\n",
      "Epoch: 407   Loss: 0.00020464394765440375\n",
      "Epoch: 408   Loss: 0.0002039976534433663\n",
      "Epoch: 409   Loss: 0.00020335348381195217\n",
      "Epoch: 410   Loss: 0.00020271171524655074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 411   Loss: 0.00020207458874210715\n",
      "Epoch: 412   Loss: 0.0002014394267462194\n",
      "Epoch: 413   Loss: 0.0002008067531278357\n",
      "Epoch: 414   Loss: 0.00020017723727505654\n",
      "Epoch: 415   Loss: 0.0001995499769691378\n",
      "Epoch: 416   Loss: 0.00019892584532499313\n",
      "Epoch: 417   Loss: 0.00019830396922770888\n",
      "Epoch: 418   Loss: 0.00019768507627304643\n",
      "Epoch: 419   Loss: 0.00019706928287632763\n",
      "Epoch: 420   Loss: 0.0001964567054528743\n",
      "Epoch: 421   Loss: 0.00019584513211157173\n",
      "Epoch: 422   Loss: 0.00019523708033375442\n",
      "Epoch: 423   Loss: 0.0001946312841027975\n",
      "Epoch: 424   Loss: 0.00019402915495447814\n",
      "Epoch: 425   Loss: 0.00019343008170835674\n",
      "Epoch: 426   Loss: 0.00019283218716736883\n",
      "Epoch: 427   Loss: 0.00019223646086174995\n",
      "Epoch: 428   Loss: 0.00019164411060046405\n",
      "Epoch: 429   Loss: 0.0001910543505800888\n",
      "Epoch: 430   Loss: 0.00019046735542360693\n",
      "Epoch: 431   Loss: 0.00018988156807608902\n",
      "Epoch: 432   Loss: 0.00018930055375676602\n",
      "Epoch: 433   Loss: 0.00018871987413149327\n",
      "Epoch: 434   Loss: 0.0001881423086160794\n",
      "Epoch: 435   Loss: 0.00018756657664198428\n",
      "Epoch: 436   Loss: 0.00018699467182159424\n",
      "Epoch: 437   Loss: 0.0001864244695752859\n",
      "Epoch: 438   Loss: 0.0001858571486081928\n",
      "Epoch: 439   Loss: 0.00018529067165218294\n",
      "Epoch: 440   Loss: 0.00018472818192094564\n",
      "Epoch: 441   Loss: 0.00018416738021187484\n",
      "Epoch: 442   Loss: 0.00018360940157435834\n",
      "Epoch: 443   Loss: 0.0001830527908168733\n",
      "Epoch: 444   Loss: 0.00018249900313094258\n",
      "Epoch: 445   Loss: 0.0001819481112761423\n",
      "Epoch: 446   Loss: 0.0001818187301978469\n",
      "Epoch: 447   Loss: 0.00018085187184624374\n",
      "Epoch: 448   Loss: 0.00018030665523838252\n",
      "Epoch: 449   Loss: 0.00017976552771870047\n",
      "Epoch: 450   Loss: 0.0001792249531717971\n",
      "Epoch: 451   Loss: 0.00017868656141217798\n",
      "Epoch: 452   Loss: 0.00017815122555475682\n",
      "Epoch: 453   Loss: 0.00017761799972504377\n",
      "Epoch: 454   Loss: 0.00017708590894471854\n",
      "Epoch: 455   Loss: 0.00017655723786447197\n",
      "Epoch: 456   Loss: 0.0001760289742378518\n",
      "Epoch: 457   Loss: 0.00017550519260112196\n",
      "Epoch: 458   Loss: 0.00017498213856015354\n",
      "Epoch: 459   Loss: 0.00017446164565626532\n",
      "Epoch: 460   Loss: 0.00017394365568179637\n",
      "Epoch: 461   Loss: 0.00017342696082778275\n",
      "Epoch: 462   Loss: 0.00017291279800701886\n",
      "Epoch: 463   Loss: 0.00017239988665096462\n",
      "Epoch: 464   Loss: 0.0001718901767162606\n",
      "Epoch: 465   Loss: 0.00017138248949777335\n",
      "Epoch: 466   Loss: 0.00017087627202272415\n",
      "Epoch: 467   Loss: 0.00017037225188687444\n",
      "Epoch: 468   Loss: 0.00016986958507914096\n",
      "Epoch: 469   Loss: 0.00016936910105869174\n",
      "Epoch: 470   Loss: 0.0001688711781753227\n",
      "Epoch: 471   Loss: 0.0001683755253907293\n",
      "Epoch: 472   Loss: 0.00016788179345894605\n",
      "Epoch: 473   Loss: 0.00016738897829782218\n",
      "Epoch: 474   Loss: 0.00016689837502781302\n",
      "Epoch: 475   Loss: 0.00016641098773106933\n",
      "Epoch: 476   Loss: 0.00016592419706285\n",
      "Epoch: 477   Loss: 0.00016543923993594944\n",
      "Epoch: 478   Loss: 0.00016495652380399406\n",
      "Epoch: 479   Loss: 0.0001644751610001549\n",
      "Epoch: 480   Loss: 0.0001639973488636315\n",
      "Epoch: 481   Loss: 0.00016352056991308928\n",
      "Epoch: 482   Loss: 0.00016304569726344198\n",
      "Epoch: 483   Loss: 0.00016257166862487793\n",
      "Epoch: 484   Loss: 0.00016209963359870017\n",
      "Epoch: 485   Loss: 0.00016163128020707518\n",
      "Epoch: 486   Loss: 0.00016116263577714562\n",
      "Epoch: 487   Loss: 0.00016069630510173738\n",
      "Epoch: 488   Loss: 0.0001602315460331738\n",
      "Epoch: 489   Loss: 0.00015976947906892747\n",
      "Epoch: 490   Loss: 0.00015930962399579585\n",
      "Epoch: 491   Loss: 0.00015884959429968148\n",
      "Epoch: 492   Loss: 0.00015839262050576508\n",
      "Epoch: 493   Loss: 0.00015793743659742177\n",
      "Epoch: 494   Loss: 0.00015748277655802667\n",
      "Epoch: 495   Loss: 0.00015703168173786253\n",
      "Epoch: 496   Loss: 0.00015658081974834204\n",
      "Epoch: 497   Loss: 0.0001561321405461058\n",
      "Epoch: 498   Loss: 0.00015568612434435636\n",
      "Epoch: 499   Loss: 0.00015524010814260691\n",
      "Epoch: 500   Loss: 0.00015479690046049654\n"
     ]
    }
   ],
   "source": [
    "# Re-initialize the optimizer, since the architecture has changed\n",
    "optim = optax.adam(1e-3)\n",
    "opt_state = optim.init(eqx.filter(pruned_network, eqx.is_array))\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 500\n",
    "key = jr.PRNGKey(0)\n",
    "for epoch in range(n_epochs):\n",
    "    *keys, key = jr.split(key, x.shape[0] + 1)\n",
    "    keys = jnp.array(keys)\n",
    "    pruned_network, opt_state, loss = step(pruned_network, opt_state, x, y, keys)\n",
    "    print(f\"Epoch: {epoch + 1}   Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training for 500 more epochs, the pruned network achieves almost the same training loss as the original network, with less than a third of the weights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "835bfa397ca6b52109e840e94dda5018b9451368d8fd0270017bd25fbadac04f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
