{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting Started \u00a4 Connex is a small JAX library built on Equinox whose aim is to incorporate artificial analogues of biological neural network attributes into deep learning research and architecture design. Currently, this includes: Complex Connectivity : Turn any directed acyclic graph (DAG) into a trainable neural network. Plasticity : Add and remove both connections and neurons at the individual level. Firing Modulation : Set and modify dropout probabilities for all neurons individually. Installation \u00a4 pip install connex Requires Python 3.7+, JAX 0.3.4+, and Equinox 0.5.2+. Usage \u00a4 Suppose we would like to create a trainable neural network from the following DAG with input neuron 0 and output neurons 3 and 11 (in that order), with a jax.nn.relu activation function for the hidden neurons: import connex as cnx import jax.nn as jnn # Specify number of neurons num_neurons = 12 # Build the adjacency dict adjacency_dict = { 0 : [ 1 , 2 , 3 ], 1 : [ 4 ], 2 : [ 4 , 5 ], 4 : [ 6 ], 5 : [ 7 ], 6 : [ 8 , 9 ], 7 : [ 10 ], 8 : [ 11 ], 9 : [ 11 ], 10 : [ 11 ] } # Specify the input and output neurons input_neurons = [ 0 ] output_neurons = [ 3 , 11 ] # Create the network network = cnx . NeuralNetwork ( num_neurons , adjacency_dict , input_neurons , output_neurons , jnn . relu ) That's it! A connex.NeuralNetwork is a subclass of equinox.Module , so it can be trained in the same fashion: import equinox as eqx import jax import jax.numpy as jnp import optax # Initialize the optimizer optim = optax . adam ( 1e-3 ) opt_state = optim . init ( eqx . filter ( network , eqx . is_array )) # Define the loss function def loss_fn ( model , x , y ): preds = jax . vmap ( model )( x ) return jnp . mean (( preds - y ) ** 2 ) # Define a single training step @eqx . filter_jit def step ( model , optim , opt_state , X_batch , y_batch ): loss , grads = eqx . filter_value_and_grad ( loss_fn )( model , X_batch , y_batch ) updates , opt_state = optim . update ( grads , opt_state , model ) model = eqx . apply_updates ( model , updates ) return model , opt_state , loss # Toy data X = jnp . expand_dims ( jnp . linspace ( 0 , 2 * jnp . pi , 250 ), 1 ) y = jnp . hstack (( jnp . cos ( X ), jnp . sin ( X ))) # Training loop n_epochs = 1000 for _ in range ( n_epochs ): network , opt_state , loss = step ( network , optim , opt_state , X , y ) Now suppose we wish to add connections 1 \u2192 6 and 2 \u2192 11, remove neuron 9, and set the dropout probability of all hidden neurons to 0.1: # Add connections network = cnx . add_connections ( network , [( 1 , 6 ), ( 2 , 11 )]) # Remove neuron network , _ = cnx . remove_neurons ( network , [ 9 ]) # Set dropout probability network . set_dropout_p ( 0.1 ) That's all there is to it. The parameters have been retained for neurons in the original network that have not been removed. connex.remove_neurons also returns auxiliary information about neuron ids, since removal of neurons re-numbers the remaining ones. For more information about manipulating connectivity structure and the NeuralNetwork base class, please see the API section of the documentation. For examples of subclassing NeuralNetwork , please see connex.nn . Feedback is greatly appeciated!","title":"Getting Started"},{"location":"#getting-started","text":"Connex is a small JAX library built on Equinox whose aim is to incorporate artificial analogues of biological neural network attributes into deep learning research and architecture design. Currently, this includes: Complex Connectivity : Turn any directed acyclic graph (DAG) into a trainable neural network. Plasticity : Add and remove both connections and neurons at the individual level. Firing Modulation : Set and modify dropout probabilities for all neurons individually.","title":"Getting Started"},{"location":"#installation","text":"pip install connex Requires Python 3.7+, JAX 0.3.4+, and Equinox 0.5.2+.","title":"Installation"},{"location":"#usage","text":"Suppose we would like to create a trainable neural network from the following DAG with input neuron 0 and output neurons 3 and 11 (in that order), with a jax.nn.relu activation function for the hidden neurons: import connex as cnx import jax.nn as jnn # Specify number of neurons num_neurons = 12 # Build the adjacency dict adjacency_dict = { 0 : [ 1 , 2 , 3 ], 1 : [ 4 ], 2 : [ 4 , 5 ], 4 : [ 6 ], 5 : [ 7 ], 6 : [ 8 , 9 ], 7 : [ 10 ], 8 : [ 11 ], 9 : [ 11 ], 10 : [ 11 ] } # Specify the input and output neurons input_neurons = [ 0 ] output_neurons = [ 3 , 11 ] # Create the network network = cnx . NeuralNetwork ( num_neurons , adjacency_dict , input_neurons , output_neurons , jnn . relu ) That's it! A connex.NeuralNetwork is a subclass of equinox.Module , so it can be trained in the same fashion: import equinox as eqx import jax import jax.numpy as jnp import optax # Initialize the optimizer optim = optax . adam ( 1e-3 ) opt_state = optim . init ( eqx . filter ( network , eqx . is_array )) # Define the loss function def loss_fn ( model , x , y ): preds = jax . vmap ( model )( x ) return jnp . mean (( preds - y ) ** 2 ) # Define a single training step @eqx . filter_jit def step ( model , optim , opt_state , X_batch , y_batch ): loss , grads = eqx . filter_value_and_grad ( loss_fn )( model , X_batch , y_batch ) updates , opt_state = optim . update ( grads , opt_state , model ) model = eqx . apply_updates ( model , updates ) return model , opt_state , loss # Toy data X = jnp . expand_dims ( jnp . linspace ( 0 , 2 * jnp . pi , 250 ), 1 ) y = jnp . hstack (( jnp . cos ( X ), jnp . sin ( X ))) # Training loop n_epochs = 1000 for _ in range ( n_epochs ): network , opt_state , loss = step ( network , optim , opt_state , X , y ) Now suppose we wish to add connections 1 \u2192 6 and 2 \u2192 11, remove neuron 9, and set the dropout probability of all hidden neurons to 0.1: # Add connections network = cnx . add_connections ( network , [( 1 , 6 ), ( 2 , 11 )]) # Remove neuron network , _ = cnx . remove_neurons ( network , [ 9 ]) # Set dropout probability network . set_dropout_p ( 0.1 ) That's all there is to it. The parameters have been retained for neurons in the original network that have not been removed. connex.remove_neurons also returns auxiliary information about neuron ids, since removal of neurons re-numbers the remaining ones. For more information about manipulating connectivity structure and the NeuralNetwork base class, please see the API section of the documentation. For examples of subclassing NeuralNetwork , please see connex.nn . Feedback is greatly appeciated!","title":"Usage"},{"location":"api/network/","text":"Neural Network \u00a4 The main class/object of the connex library. connex.NeuralNetwork \u00a4 A neural network whose structure is primarily specified by an adjecency matrix representing a directed acyclic graph (DAG) and sequences of ints specifying which neurons are input and output neurons. Create your model by inheriting from this. __init__ ( self , num_neurons : int , adjacency_dict : Mapping [ int , Sequence [ int ]], input_neurons : Sequence [ int ], output_neurons : Sequence [ int ], activation : Callable = < function silu > , output_activation : Callable = < function _identity > , dropout_p : Union [ float , Sequence [ float ]] = 0.0 , seed : int = 0 , parameter_matrix : Optional [ Array ] = None , ** kwargs ) \u00a4 Arguments: num_neurons : The number of neurons in the network. adjacency_dict : A dictionary that maps a neuron id to the ids of its outputs. Neurons must be ordered from 0 to num_neurons - 1 . Neurons with no outgoing connections do not need to be included. input_neurons : A sequence of int indicating the ids of the input neurons. The order here matters, as the input data will be passed into the input neurons in the order passed in here. output_neurons : A sequence of int indicating the ids of the output neurons. The order here matters, as the output data will be read from the output neurons in the order passed in here. activation : The activation function applied element-wise to the hidden (i.e. non-input, non-output) neurons. It can itself be a trainable equinox.Module . output_activation : The activation function applied element-wise to the output neurons. It can itself be a trainable equinox.Module . dropout_p : Dropout probability. If a single float , the same dropout probability will be applied to all hidden neurons. If a Sequence[float] , the sequence must have length num_neurons , where dropout_p[i] is the dropout probability for neuron i . Note that this allows dropout to be applied to input and output neurons as well. seed : The random seed used to initialize parameters. parameter_matrix : A jnp.array of shape (N, N + 1) where entry [i, j] is neuron i 's weight for neuron j , and entry [i, N] is neuron i 's bias. Optional argument -- used primarily for plasticity functionality. __call__ ( self , x : Array ) -> Array \u00a4 The forward pass of the network. Neurons are \"fired\" in topological batch order (see https://arxiv.org/pdf/1911.06904.pdf), with jax.vmap vectorization used within each topological batch. Arguments : x : The input array to the network for the forward pass. The individual values will be written to the input neurons in the order passed in during initialization. Returns : The result array from the forward pass. The order of the array elements will be the order of the output neurons passed in during initialization. get_dropout_p ( self ) -> Array \u00a4 Get the per-neuron dropout probabilities. Returns : A 1D jnp.array with shape (num_neurons,) where element i is the dropout probability of neuron i . set_dropout_p ( self , dropout_p : Union [ float , Sequence [ float ]]) -> None \u00a4 Set the per-neuron dropout probabilities. Arguments : dropout_p : Dropout probability. If a single float , the same dropout probability will be applied to all hidden neurons. If a Sequence[float] , the sequence must have length num_neurons , where dropout_p[i] is the dropout probability for neuron i . Note that this allows dropout to be applied to input and output neurons as well.","title":"Neural Network"},{"location":"api/network/#neural-network","text":"The main class/object of the connex library.","title":"Neural Network"},{"location":"api/network/#connex.NeuralNetwork","text":"A neural network whose structure is primarily specified by an adjecency matrix representing a directed acyclic graph (DAG) and sequences of ints specifying which neurons are input and output neurons. Create your model by inheriting from this.","title":"NeuralNetwork"},{"location":"api/plasticity/","text":"Artificial Neuroplasticity \u00a4 The brain has a remarkable ability to rewire itself under the right conditions, known as plasticity . This includes, among other processes, synaptogenesis (the formation of new synapses), synaptic pruning (the removal of synapses), neurogenesis (the formation of new neurons), and programmed cell death (the removal of neurons), all of which have been shown to play prominent roles during various parts of neural development. We provide the following code functionality to mirror these processes. All leave the input network(s) unmodified. connex . add_connections ( network : NeuralNetwork , connections : Sequence [ Tuple [ int , int ]], input_neurons : Optional [ Sequence [ int ]] = None , output_neurons : Optional [ Sequence [ int ]] = None , dropout_p : Union [ float , Sequence [ float ]] = None ) -> NeuralNetwork \u00a4 Add connections to the network. Arguments : network : A NeuralNetwork object connections : A sequence of pairs (i, j) to add to the network as directed edges. input_neurons : A sequence of int indicating the ids of the input neurons. The order here matters, as the input data will be passed into the input neurons in the order passed in here. Optional argument. If None , the input neurons of the original network will be retained. output_neurons : A sequence of int indicating the ids of the output neurons. The order here matters, as the output data will be read from the output neurons in the order passed in here. Optional argument. If None , the output neurons of the original network will be retained. dropout_p : Dropout probability. If a single float , the same dropout probability will be applied to all hidden neurons. If a Sequence[float] , the sequence must have length num_neurons , where dropout_p[i] is the dropout probability for neuron i . Note that this allows dropout to be applied to input and output neurons as well. Optional argument. If None , the dropout probabilities of the original network will be retained. Returns : A NeuralNetwork object with the input connections added and original parameters retained. connex . remove_connections ( network : NeuralNetwork , connections : Sequence [ Tuple [ int , int ]], input_neurons : Optional [ Sequence [ int ]] = None , output_neurons : Optional [ Sequence [ int ]] = None , dropout_p : Union [ float , Sequence [ float ]] = None ) -> NeuralNetwork \u00a4 Remove connections from the network. Arguments : network : A NeuralNetwork object connections : A sequence of pairs (i, j) to remove from the network as directed edges. input_neurons : A sequence of int indicating the ids of the input neurons. The order here matters, as the input data will be passed into the input neurons in the order passed in here. Optional argument. If None , the input neurons of the original network will be retained. output_neurons : A sequence of int indicating the ids of the output neurons. The order here matters, as the output data will be read from the output neurons in the order passed in here. Optional argument. If None , the output neurons of the original network will be retained. dropout_p : Dropout probability. If a single float , the same dropout probability will be applied to all hidden neurons. If a Sequence[float] , the sequence must have length num_neurons , where dropout_p[i] is the dropout probability for neuron i . Note that this allows dropout to be applied to input and output neurons as well. Optional argument. If None , the dropout probabilities of the original network will be retained. Returns : A NeuralNetwork object with the desired connections removed and original parameters retained. connex . add_neurons ( network : NeuralNetwork , new_neuron_data : Sequence [ Mapping ]) -> Tuple [ NeuralNetwork , Sequence [ int ]] \u00a4 Add neurons to the network. These can be input, hidden, or output neurons. Arguments : network : A NeuralNetwork object new_neuron_data : A sequence of dictionaries, where each dictionary represents a new neuron to add to the network. Each dictionary must have 4 str fields: 'in_neurons' : An Optional[Sequence[int]] indexing the neurons from the original network that feed into the new neuron. 'out_neurons' : An Optional[Sequence[int]] indexing the neurons from the original network which the new neuron feeds into. 'type' : One of {'input', 'hidden', 'output'} . A str representing which group the new neuron belongs to. 'dropout_p' : An Optional[float] , the dropout probability for the new neuron. Defaults to 0. Returns : A 2-tuple where the first element is the new NeuralNetwork with the new neurons added and parameters from original neurons retained, and the second element is the sequence of the ids assigned to the added neurons in the order they were passed in through the input argument new_neuron_data . connex . remove_neurons ( network : NeuralNetwork , ids : Sequence [ int ]) -> Tuple [ NeuralNetwork , Dict [ int , int ]] \u00a4 Remove neurons from the network. These can be input, hidden, or output neurons. Arguments : network : A NeuralNetwork object. ids : A sequence of int ids corresponding to the neurons to remove from the network. Returns : A 2-tuple where the first element is the new NeuralNetwork with the desired neurons removed (along with all respective incoming and outgoing connections) and parameters from original neurons retained, and the second element is a dictionary mapping neuron ids from the original network to their respective ids in the new network. connex . connect_networks ( network1 : NeuralNetwork , network2 : NeuralNetwork , connection_map_1_to_2 : Mapping [ int , Sequence [ int ]] = {}, connection_map_2_to_1 : Mapping [ int , Sequence [ int ]] = {}, input_neurons : Optional [ Tuple [ Sequence [ int ], Sequence [ int ]]] = None , output_neurons : Optional [ Tuple [ Sequence [ int ], Sequence [ int ]]] = None , activation : Callable = < function silu > , output_activation : Callable = < function _identity > , dropout_p : Union [ float , Sequence [ float ]] = None , seed : int = 42 , keep_parameters : bool = True ) -> Tuple [ NeuralNetwork , Dict [ int , int ]] \u00a4 Connect two networks together in a specified manner. Arguments : network1 : A NeuralNetwork object. network2 : A NeuralNetwork object. connection_map_1_to_2 A dictionary that maps an int id representing the corresponding neuron in network1 to a sequence of int ids representing the corresponding neurons in network2 to which to connect the network1 neuron. connection_map_2_to_1 A dictionary that maps an int id representing the corresponding neuron in network2 to a sequence of int ids representing the corresponding neurons in network1 to which to connect the network2 neuron. input_neurons : A 2-tuple of int sequences, where the first sequence is ids of network1 neurons and the second sequence is ids of network2 neurons. The two sequences will be concatenated (and appropriately re-numbered) to form the input neurons of the new network. Optional argument. If None , the input neurons of the new network will be the concatenation of the input neurons of network1 and network2 . output_neurons : A 2-tuple of int sequences, where the first sequence is ids of network1 neurons and the second sequence is ids of network2 neurons. The two sequences will be concatenated (and appropriately re-numbered) to form the output neurons of the new network. Optional argument. If None , the output neurons of the new network will be the concatenation of the output neurons of network1 and network2 . activation : The activation function applied element-wise to the hidden (i.e. non-input, non-output) neurons. It can itself be a trainable equinox.Module . output_activation : The activation function applied element-wise to the output neurons. It can itself be a trainable equinox.Module . dropout_p : Dropout probability. If a single float , the same dropout probability will be applied to all hidden neurons. If a Sequence[float] , the sequence must have length num_neurons , where dropout_p[i] is the dropout probability for neuron i . Note that this allows dropout to be applied to input and output neurons as well. Optional argument. If None , defaults to the concatenation of network1.get_dropout_p() and network2.get_dropout_p() . seed : The random seed used to initialize parameters. keep_parameters : If True , copies the parameters of network1 and network2 to the appropriate parameter entries of the new network. Returns : A 2-tuple where the first element is the new NeuralNetwork , and the second element is A dictionary mapping neuron ids from network2 to their respective ids in the new network. The network1 ids are left unchanged.","title":"Artificial Neuroplasticity"},{"location":"api/plasticity/#artificial-neuroplasticity","text":"The brain has a remarkable ability to rewire itself under the right conditions, known as plasticity . This includes, among other processes, synaptogenesis (the formation of new synapses), synaptic pruning (the removal of synapses), neurogenesis (the formation of new neurons), and programmed cell death (the removal of neurons), all of which have been shown to play prominent roles during various parts of neural development. We provide the following code functionality to mirror these processes. All leave the input network(s) unmodified.","title":"Artificial Neuroplasticity"},{"location":"api/plasticity/#connex.add_connections","text":"Add connections to the network. Arguments : network : A NeuralNetwork object connections : A sequence of pairs (i, j) to add to the network as directed edges. input_neurons : A sequence of int indicating the ids of the input neurons. The order here matters, as the input data will be passed into the input neurons in the order passed in here. Optional argument. If None , the input neurons of the original network will be retained. output_neurons : A sequence of int indicating the ids of the output neurons. The order here matters, as the output data will be read from the output neurons in the order passed in here. Optional argument. If None , the output neurons of the original network will be retained. dropout_p : Dropout probability. If a single float , the same dropout probability will be applied to all hidden neurons. If a Sequence[float] , the sequence must have length num_neurons , where dropout_p[i] is the dropout probability for neuron i . Note that this allows dropout to be applied to input and output neurons as well. Optional argument. If None , the dropout probabilities of the original network will be retained. Returns : A NeuralNetwork object with the input connections added and original parameters retained.","title":"add_connections()"},{"location":"api/plasticity/#connex.remove_connections","text":"Remove connections from the network. Arguments : network : A NeuralNetwork object connections : A sequence of pairs (i, j) to remove from the network as directed edges. input_neurons : A sequence of int indicating the ids of the input neurons. The order here matters, as the input data will be passed into the input neurons in the order passed in here. Optional argument. If None , the input neurons of the original network will be retained. output_neurons : A sequence of int indicating the ids of the output neurons. The order here matters, as the output data will be read from the output neurons in the order passed in here. Optional argument. If None , the output neurons of the original network will be retained. dropout_p : Dropout probability. If a single float , the same dropout probability will be applied to all hidden neurons. If a Sequence[float] , the sequence must have length num_neurons , where dropout_p[i] is the dropout probability for neuron i . Note that this allows dropout to be applied to input and output neurons as well. Optional argument. If None , the dropout probabilities of the original network will be retained. Returns : A NeuralNetwork object with the desired connections removed and original parameters retained.","title":"remove_connections()"},{"location":"api/plasticity/#connex.add_neurons","text":"Add neurons to the network. These can be input, hidden, or output neurons. Arguments : network : A NeuralNetwork object new_neuron_data : A sequence of dictionaries, where each dictionary represents a new neuron to add to the network. Each dictionary must have 4 str fields: 'in_neurons' : An Optional[Sequence[int]] indexing the neurons from the original network that feed into the new neuron. 'out_neurons' : An Optional[Sequence[int]] indexing the neurons from the original network which the new neuron feeds into. 'type' : One of {'input', 'hidden', 'output'} . A str representing which group the new neuron belongs to. 'dropout_p' : An Optional[float] , the dropout probability for the new neuron. Defaults to 0. Returns : A 2-tuple where the first element is the new NeuralNetwork with the new neurons added and parameters from original neurons retained, and the second element is the sequence of the ids assigned to the added neurons in the order they were passed in through the input argument new_neuron_data .","title":"add_neurons()"},{"location":"api/plasticity/#connex.remove_neurons","text":"Remove neurons from the network. These can be input, hidden, or output neurons. Arguments : network : A NeuralNetwork object. ids : A sequence of int ids corresponding to the neurons to remove from the network. Returns : A 2-tuple where the first element is the new NeuralNetwork with the desired neurons removed (along with all respective incoming and outgoing connections) and parameters from original neurons retained, and the second element is a dictionary mapping neuron ids from the original network to their respective ids in the new network.","title":"remove_neurons()"},{"location":"api/plasticity/#connex.connect_networks","text":"Connect two networks together in a specified manner. Arguments : network1 : A NeuralNetwork object. network2 : A NeuralNetwork object. connection_map_1_to_2 A dictionary that maps an int id representing the corresponding neuron in network1 to a sequence of int ids representing the corresponding neurons in network2 to which to connect the network1 neuron. connection_map_2_to_1 A dictionary that maps an int id representing the corresponding neuron in network2 to a sequence of int ids representing the corresponding neurons in network1 to which to connect the network2 neuron. input_neurons : A 2-tuple of int sequences, where the first sequence is ids of network1 neurons and the second sequence is ids of network2 neurons. The two sequences will be concatenated (and appropriately re-numbered) to form the input neurons of the new network. Optional argument. If None , the input neurons of the new network will be the concatenation of the input neurons of network1 and network2 . output_neurons : A 2-tuple of int sequences, where the first sequence is ids of network1 neurons and the second sequence is ids of network2 neurons. The two sequences will be concatenated (and appropriately re-numbered) to form the output neurons of the new network. Optional argument. If None , the output neurons of the new network will be the concatenation of the output neurons of network1 and network2 . activation : The activation function applied element-wise to the hidden (i.e. non-input, non-output) neurons. It can itself be a trainable equinox.Module . output_activation : The activation function applied element-wise to the output neurons. It can itself be a trainable equinox.Module . dropout_p : Dropout probability. If a single float , the same dropout probability will be applied to all hidden neurons. If a Sequence[float] , the sequence must have length num_neurons , where dropout_p[i] is the dropout probability for neuron i . Note that this allows dropout to be applied to input and output neurons as well. Optional argument. If None , defaults to the concatenation of network1.get_dropout_p() and network2.get_dropout_p() . seed : The random seed used to initialize parameters. keep_parameters : If True , copies the parameters of network1 and network2 to the appropriate parameter entries of the new network. Returns : A 2-tuple where the first element is the new NeuralNetwork , and the second element is A dictionary mapping neuron ids from network2 to their respective ids in the new network. The network1 ids are left unchanged.","title":"connect_networks()"},{"location":"api/nn/mcmlp/","text":"MCMLP \u00a4 connex.nn.MCMLP ( NeuralNetwork ) \u00a4 A \"Maximally-Connected Multi-Layer Perceptron\". Like a standard MLP, but every neuron is connected to every other neuron in all later layers, rather than only the next layer. __init__ ( self , input_size : int , output_size : int , width : int , depth : int , activation : Callable = < function silu > , output_activation : Callable = < function _identity > , seed : int = 0 , ** kwargs ) \u00a4 Arguments : input_size : The number of neurons in the input layer. output_size : The number of neurons in the output layer. width : The number of neurons in each hidden layer. depth : The number of hidden layers. activation : The activation function applied element-wise to the hidden (i.e. non-input, non-output) neurons. It can itself be a trainable equinox Module. output_activation : The activation function applied element-wise to the output neurons. It can itself be a trainable equinox Module. seed : The random seed used to initialize parameters.","title":"MCMLP"},{"location":"api/nn/mcmlp/#mcmlp","text":"","title":"MCMLP"},{"location":"api/nn/mcmlp/#connex.nn.MCMLP","text":"A \"Maximally-Connected Multi-Layer Perceptron\". Like a standard MLP, but every neuron is connected to every other neuron in all later layers, rather than only the next layer.","title":"MCMLP"},{"location":"api/nn/mlp/","text":"MLP \u00a4 connex.nn.MLP ( NeuralNetwork ) \u00a4 A standard Multi-Layer Perceptron with constant layer width. __init__ ( self , input_size : int , output_size : int , width : int , depth : int , activation : Callable = < function silu > , output_activation : Callable = < function _identity > , seed : int = 0 , ** kwargs ) \u00a4 Arguments : input_size : The number of neurons in the input layer. output_size : The number of neurons in the output layer. width : The number of neurons in each hidden layer. depth : The number of hidden layers. activation : The activation function applied element-wise to the hidden (i.e. non-input, non-output) neurons. It can itself be a trainable equinox Module. output_activation : The activation function applied element-wise to the output neurons. It can itself be a trainable equinox Module. seed : The random seed used to initialize parameters.","title":"MLP"},{"location":"api/nn/mlp/#mlp","text":"","title":"MLP"},{"location":"api/nn/mlp/#connex.nn.MLP","text":"A standard Multi-Layer Perceptron with constant layer width.","title":"MLP"}]}