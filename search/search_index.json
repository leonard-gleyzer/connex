{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting Started \u00a4 Connex is a JAX library built on Equinox that allows for fine-grained, dynamic control of neural network topology. With Connex, you can Turn any directed acyclic graph (DAG) into a trainable neural network. Add and remove both connections and neurons at the individual level. Set and modify dropout probabilities for all neurons individually. Easily toggle techniques such as normalization, adaptive activations, and self-attention. Export a trained network to a NetworkX weighted digraph for network analysis. Installation \u00a4 pip install connex Usage \u00a4 As a tiny pedagogical example, let's create a trainable neural network from the following DAG with input neuron 0 and output neurons 3 and 11 (in that order) and ReLU activation for the hidden neurons: import connex as cnx import jax.nn as jnn # Create the graph data adjacency_dict = { 0 : [ 1 , 2 , 3 ], 1 : [ 4 ], 2 : [ 4 , 5 ], 4 : [ 6 ], 5 : [ 7 ], 6 : [ 8 , 9 ], 7 : [ 10 ], 8 : [ 11 ], 9 : [ 11 ], 10 : [ 11 ] } # Specify the input and output neurons input_neurons = [ 0 ] output_neurons = [ 3 , 11 ] # Create the network network = cnx . NeuralNetwork ( adjacency_dict , input_neurons , output_neurons , jnn . relu ) That's it! A connex.NeuralNetwork is a subclass of equinox.Module , so it can be trained in the same fashion: import equinox as eqx import jax import jax.numpy as jnp import jax.random as jr import optax # Initialize the optimizer optim = optax . adam ( 1e-3 ) opt_state = optim . init ( eqx . filter ( network , eqx . is_array )) # Define the loss function @eqx . filter_value_and_grad def loss_fn ( model , x , y ): preds = jax . vmap ( model )( x ) return jnp . mean (( preds - y ) ** 2 ) # Define a single training step @eqx . filter_jit def step ( model , opt_state , x , y ): loss , grads = loss_fn ( model , x , y ) updates , opt_state = optim . update ( grads , opt_state , model ) model = eqx . apply_updates ( model , updates ) return model , opt_state , loss # Toy data x = jnp . expand_dims ( jnp . linspace ( 0 , 2 * jnp . pi , 250 ), 1 ) y = jnp . hstack (( jnp . cos ( x ), jnp . sin ( x ))) # Training loop n_epochs = 500 key = jr . PRNGKey ( 0 ) for epoch in range ( n_epochs ): network , opt_state , loss = step ( network , opt_state , x , y ) print ( f \"Epoch: { epoch + 1 } Loss: { loss } \" ) Now suppose we wish to add connections 1 \u2192 6 and 2 \u2192 11, remove neuron 9, and set the dropout probability of all hidden neurons to 0.1: # Add connections network = cnx . add_connections ( network , [( 1 , 6 ), ( 2 , 11 )]) # Remove neuron network = cnx . remove_neurons ( network , [ 9 ]) # Set dropout probability network = cnx . set_dropout_p ( network , 0.1 ) That's all there is to it. The new connections have been initialized with untrained parameters, and the neurons in the original network that have not been removed (along with their respective incoming and outgoing connections) have retained their trained parameters. Furthermore, since a connex.NeuralNetwork is an equinox.Module , it can seamlessly be used as a submodule inside other Equinox Modules. For more information about manipulating connectivity structure and the NeuralNetwork base class, please see the API section of the documentation. For examples of subclassing NeuralNetwork , please see connex.nn . Citation \u00a4 @software { gleyzer2023connex , author = {Leonard Gleyzer} , title = {{C}onnex: Fine-grained Control over Neural Network Topology in {JAX}} , url = {http://github.com/leonard-gleyzer/connex} , version = {0.3.0} , year = {2023} , }","title":"Getting Started"},{"location":"#getting-started","text":"Connex is a JAX library built on Equinox that allows for fine-grained, dynamic control of neural network topology. With Connex, you can Turn any directed acyclic graph (DAG) into a trainable neural network. Add and remove both connections and neurons at the individual level. Set and modify dropout probabilities for all neurons individually. Easily toggle techniques such as normalization, adaptive activations, and self-attention. Export a trained network to a NetworkX weighted digraph for network analysis.","title":"Getting Started"},{"location":"#installation","text":"pip install connex","title":"Installation"},{"location":"#usage","text":"As a tiny pedagogical example, let's create a trainable neural network from the following DAG with input neuron 0 and output neurons 3 and 11 (in that order) and ReLU activation for the hidden neurons: import connex as cnx import jax.nn as jnn # Create the graph data adjacency_dict = { 0 : [ 1 , 2 , 3 ], 1 : [ 4 ], 2 : [ 4 , 5 ], 4 : [ 6 ], 5 : [ 7 ], 6 : [ 8 , 9 ], 7 : [ 10 ], 8 : [ 11 ], 9 : [ 11 ], 10 : [ 11 ] } # Specify the input and output neurons input_neurons = [ 0 ] output_neurons = [ 3 , 11 ] # Create the network network = cnx . NeuralNetwork ( adjacency_dict , input_neurons , output_neurons , jnn . relu ) That's it! A connex.NeuralNetwork is a subclass of equinox.Module , so it can be trained in the same fashion: import equinox as eqx import jax import jax.numpy as jnp import jax.random as jr import optax # Initialize the optimizer optim = optax . adam ( 1e-3 ) opt_state = optim . init ( eqx . filter ( network , eqx . is_array )) # Define the loss function @eqx . filter_value_and_grad def loss_fn ( model , x , y ): preds = jax . vmap ( model )( x ) return jnp . mean (( preds - y ) ** 2 ) # Define a single training step @eqx . filter_jit def step ( model , opt_state , x , y ): loss , grads = loss_fn ( model , x , y ) updates , opt_state = optim . update ( grads , opt_state , model ) model = eqx . apply_updates ( model , updates ) return model , opt_state , loss # Toy data x = jnp . expand_dims ( jnp . linspace ( 0 , 2 * jnp . pi , 250 ), 1 ) y = jnp . hstack (( jnp . cos ( x ), jnp . sin ( x ))) # Training loop n_epochs = 500 key = jr . PRNGKey ( 0 ) for epoch in range ( n_epochs ): network , opt_state , loss = step ( network , opt_state , x , y ) print ( f \"Epoch: { epoch + 1 } Loss: { loss } \" ) Now suppose we wish to add connections 1 \u2192 6 and 2 \u2192 11, remove neuron 9, and set the dropout probability of all hidden neurons to 0.1: # Add connections network = cnx . add_connections ( network , [( 1 , 6 ), ( 2 , 11 )]) # Remove neuron network = cnx . remove_neurons ( network , [ 9 ]) # Set dropout probability network = cnx . set_dropout_p ( network , 0.1 ) That's all there is to it. The new connections have been initialized with untrained parameters, and the neurons in the original network that have not been removed (along with their respective incoming and outgoing connections) have retained their trained parameters. Furthermore, since a connex.NeuralNetwork is an equinox.Module , it can seamlessly be used as a submodule inside other Equinox Modules. For more information about manipulating connectivity structure and the NeuralNetwork base class, please see the API section of the documentation. For examples of subclassing NeuralNetwork , please see connex.nn .","title":"Usage"},{"location":"#citation","text":"@software { gleyzer2023connex , author = {Leonard Gleyzer} , title = {{C}onnex: Fine-grained Control over Neural Network Topology in {JAX}} , url = {http://github.com/leonard-gleyzer/connex} , version = {0.3.0} , year = {2023} , }","title":"Citation"},{"location":"citation/","text":"@software { gleyzer2023connex , author = {Leonard Gleyzer} , title = {{C}onnex: Fine-grained Control over Neural Network Topology in {JAX}} , url = {http://github.com/leonard-gleyzer/connex} , version = {0.3.0} , year = {2023} , }","title":"Citation"},{"location":"api/network/","text":"Neural Network \u00a4 The main object of the connex library. connex.NeuralNetwork \u00a4 A neural network whose structure is specified by a DAG. __init__ ( self , graph_data : Any , input_neurons : Sequence [ Any ], output_neurons : Sequence [ Any ], hidden_activation : Callable = < function gelu > , output_transformation : Callable = < function _identity > , dropout_p : Union [ float , Mapping [ Any , float ]] = 0.0 , use_topo_norm : bool = False , use_topo_self_attention : bool = False , use_neuron_self_attention : bool = False , use_adaptive_activations : bool = False , topo_sort : Optional [ Sequence [ Any ]] = None , * , key : Optional [ PRNGKey ] = None ) \u00a4 Arguments : graph_data : A networkx.DiGraph , or data that can be turned into a networkx.DiGraph by calling networkx.DiGraph(graph_data) (such as an adjacency dict) representing the DAG structure of the neural network. All nodes of the graph must have the same type. input_neurons : An Sequence of nodes from graph indicating the input neurons. The order here matters, as the input data will be passed into the input neurons in the order specified here. output_neurons : An Sequence of nodes from graph indicating the output neurons. The order here matters, as the output data will be read from the output neurons in the order specified here. hidden_activation : The activation function applied element-wise to the hidden (i.e. non-input, non-output) neurons. It can itself be a trainable equinox.Module . output_transformation : The transformation applied group-wise to the output neurons, e.g. jax.nn.softmax . It can itself be a trainable equinox.Module . dropout_p : Dropout probability. If a single float , the same dropout probability will be applied to all hidden neurons. If a Mapping[Any, float] , dropout_p[i] refers to the dropout probability of neuron i . All neurons default to zero unless otherwise specified. Note that this allows dropout to be applied to input and output neurons as well. use_topo_norm : A bool indicating whether to apply a topological batch- version of Layer Norm, Cite Layer Normalization @article { ba2016layer , author = {Jimmy Lei Ba, Jamie Ryan Kriso, Geoffrey E. Hinton} , title = {Layer Normalization} , year = {2016} , journal = {arXiv:1607.06450} , } where the collective inputs of each topological batch are standardized (made to have mean 0 and variance 1), with learnable elementwise-affine parameters gamma and beta . - use_topo_self_attention : A bool indicating whether to apply (single-headed) self-attention to each topological batch's collective inputs. Cite Attention is All You Need @inproceedings { vaswani2017attention , author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\\L}ukasz and Polosukhin, Illia} , booktitle = {Advances in Neural Information Processing Systems} , publisher = {Curran Associates, Inc.} , title = {Attention is All You Need} , volume = {30} , year = {2017} } use_neuron_self_attention : A bool indicating whether to apply neuron-wise self-attention, where each neuron applies (single-headed) self-attention to its inputs. If both _use_neuron_self_attention and use_neuron_norm are True , normalization is applied before self-attention. Warning Neuron-level self-attention will use significantly more memory than than topo-level self-attention. use_adaptive_activations : A bool indicating whether to use neuron-wise adaptive activations, where all hidden activations transform as \u03c3(x) -> a * \u03c3(b * x) , where a , b are trainable scalar parameters unique to each neuron. Cite Locally adaptive activation functions with slope recovery term for deep and physics-informed neural networks # noqa: E501 @article { Jagtap_2020 , author = {Ameya D. Jagtap, Kenji Kawaguchi, George Em Karniadakis} , title = {Locally adaptive activation functions with slope recovery term for deep and physics-informed neural networks} , year = {2020} , publisher = {The Royal Society} , journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences} , } topo_sort : An optional sequence of neurons indicating a topological sort of the graph. If None , a topological sort will be performed on the graph, which may be time-consuming for some networks. key : The jax.random.PRNGKey used for parameter initialization and dropout. Optional, keyword-only argument. Defaults to jax.random.PRNGKey(0) . __call__ ( self , x : Array , * , key : Optional [ PRNGKey ] = None ) -> Array \u00a4 The forward pass of the network. Neurons are \"fired\" in topological batch order -- see Section 2.2 of Cite Directed Acyclic Graph Neural Networks @inproceedings { thost2021directed , author = {Veronika Thost and Jie Chen} , booktitle = {International Conference on Learning Representations} , publisher = {Curran Associates, Inc.} , title = {Directed Acyclic Graph Neural Networks} , year = {2021} } Arguments : x : The input array to the network for the forward pass. The individual values will be written to the input neurons in the order passed in during initialization. key : A jax.random.PRNGKey used for dropout. Optional, keyword-only argument. If None , a key will be generated using the current time. Returns : The result array from the forward pass. The order of the array elements will be the order of the output neurons passed in during initialization. to_networkx_weighted_digraph ( self ) -> DiGraph \u00a4 Returns a networkx.DiGraph represention of the network with neuron weights saved as edge attributes.","title":"Neural Network"},{"location":"api/network/#neural-network","text":"The main object of the connex library.","title":"Neural Network"},{"location":"api/network/#connex.NeuralNetwork","text":"A neural network whose structure is specified by a DAG.","title":"NeuralNetwork"},{"location":"api/plasticity/","text":"Artificial Neuroplasticity \u00a4 The brain has a remarkable ability to rewire itself under the right conditions, known as plasticity. This includes, among other processes, the formation of new synapses (synaptogenesis), the removal of synapses (synaptic pruning), the formation of new neurons (neurogenesis), and the removal of neurons (programmed cell death). Furthermore, specific neurons/clusters of neurons can be made to be more or less likely to fire, known as neuromodulation. We provide the following code functionality to mirror these processes. All return a copy of the network and leave the input network unmodified. connex . add_connections ( network : NeuralNetwork , connections : Union [ Sequence [ Tuple [ Any , Any ]], Mapping [ Any , Sequence [ Any ]]], * , key : Optional [ PRNGKey ] = None ) -> NeuralNetwork \u00a4 Add connections to the network. Arguments: network : A NeuralNetwork object. connections : The directed edges to add. Must be a sequence of 2-tuples, or an adjacency dict mapping an existing neuron (by its NetworkX id) to its new outgoing connections. Connections that already exist are ignored. key : The jax.random.PRNGKey used for new weight initialization. Optional, keyword-only argument. Defaults to jax.random.PRNGKey(0) . Returns: A NeuralNetwork object with the specified connections added and original parameters retained. connex . remove_connections ( network : NeuralNetwork , connections : Union [ Sequence [ Tuple [ Any , Any ]], Mapping [ Any , Sequence [ Any ]]]) -> NeuralNetwork \u00a4 Remove connections from the network. Arguments: network : A NeuralNetwork object. connections : The directed edges to remove. Must be a sequence of 2-tuples, or an adjacency dict mapping an existing neuron (by its NetworkX id) to its new outgoing connections. Connections that already exist are ignored. Returns: A NeuralNetwork object with the specified connections removed and original parameters retained. connex . add_input_neurons ( network : NeuralNetwork , new_input_neurons : Sequence [ Any ], * , key : Optional [ PRNGKey ] = None ) -> NeuralNetwork \u00a4 Add input neurons to the network. Note that this function only adds neurons themselves, not any connections associated with the new neurons, effectively adding them as isolated nodes in the graph. Use connex.add_connections after this function has been called to add the desired connections. Arguments: network : The NeuralNetwork to add neurons to new_input_neurons : A sequence of new input neurons (more specifically, their identifiers/names) to add to the network. These must be unique, i.e. cannot already exist in the network. These must also specifically be input neurons. To add hidden or output neurons, use connex.add_hidden_neurons or connex.add_output_neurons . key : The jax.random.PRNGKey used for new parameter initialization. Optional, keyword-only argument. Defaults to jax.random.PRNGKey(0) . Returns: A NeuralNetwork with the new input neurons added and parameters from the original network retained. The new input neurons are added before the existing input neurons. For example, if the previous input neurons were [0, 1] and new input neurons [2, 3] were added, the new input neurons would be [2, 3, 0, 1] . connex . add_hidden_neurons ( network : NeuralNetwork , new_hidden_neurons : Sequence [ Any ], * , key : Optional [ PRNGKey ] = None ) -> NeuralNetwork \u00a4 Add hidden neurons to the network. Note that this function only adds neurons themselves, not any connections associated with the new neurons, effectively adding them as isolated nodes in the graph. Use connex.add_connections after this function has been called to add the desired connections. Arguments: network : The NeuralNetwork to add neurons to new_hidden_neurons : A sequence of new hidden neurons (more specifically, their identifiers/names) to add to the network. These must be unique, i.e. cannot already exist in the These must also specifically be hidden neurons. To add input or output neurons, use connex.add_input_neurons or connex.add_output_neurons . key : The jax.random.PRNGKey used for new parameter initialization. Optional, keyword-only argument. Defaults to jax.random.PRNGKey(0) . Returns: A NeuralNetwork with the new hidden neurons added and parameters from the original network retained. connex . add_output_neurons ( network : NeuralNetwork , new_output_neurons : Sequence [ Any ], * , key : Optional [ PRNGKey ] = None ) -> NeuralNetwork \u00a4 Add output neurons to the network. Note that this function only adds neurons themselves, not any connections associated with the new neurons, effectively adding them as isolated nodes in the graph. Use connex.add_connections after this function has been called to add any desired connections. Arguments: network : The NeuralNetwork to add neurons to new_output_neurons : A sequence of new output neurons (more specifically, their identifiers/names) to add to the network. These must be unique, i.e. cannot already exist in the network. These must also specifically be output neurons. To add input or output neurons, use connex.add_input_neurons or connex.add_output_neurons . key : The jax.random.PRNGKey used for new parameter initialization. Optional, keyword-only argument. Defaults to jax.random.PRNGKey(0) . Returns: A NeuralNetwork with the new output neurons added and parameters from the original network retained. connex . remove_neurons ( network : NeuralNetwork , neurons : Sequence [ Any ]) -> NeuralNetwork \u00a4 Remove neurons and any of their incoming/outgoing connections from the network. Arguments: network : The NeuralNetwork to add neurons to neurons : A sequence of neurons (more specifically, their identifiers/names) to remove from the network. These can be input, hidden, or output neurons. Returns: A NeuralNetwork with the specified neurons removed and parameters from the original network retained. connex . set_dropout_p ( network : NeuralNetwork , dropout_p : Union [ float , Mapping [ Any , float ]]) -> NeuralNetwork \u00a4 Set the per-neuron dropout probabilities. Arguments: network : The NeuralNetwork whose dropout probabilities will be modified. dropout_p : Either a float or mapping from neuron ( Any ) to float. If a single float, all hidden neurons will have that dropout probability, and all input and output neurons will have dropout probability 0 by default. If a Mapping , it is assumed that dropout_p maps a neuron to its dropout probability, and all unspecified neurons will retain their current dropout probability. Returns: A copy of the network with dropout probabilities as specified. The original network (including unspecified dropout probabilities) is left unchanged.","title":"Artificial Neuroplasticity"},{"location":"api/plasticity/#artificial-neuroplasticity","text":"The brain has a remarkable ability to rewire itself under the right conditions, known as plasticity. This includes, among other processes, the formation of new synapses (synaptogenesis), the removal of synapses (synaptic pruning), the formation of new neurons (neurogenesis), and the removal of neurons (programmed cell death). Furthermore, specific neurons/clusters of neurons can be made to be more or less likely to fire, known as neuromodulation. We provide the following code functionality to mirror these processes. All return a copy of the network and leave the input network unmodified.","title":"Artificial Neuroplasticity"},{"location":"api/plasticity/#connex.add_connections","text":"Add connections to the network. Arguments: network : A NeuralNetwork object. connections : The directed edges to add. Must be a sequence of 2-tuples, or an adjacency dict mapping an existing neuron (by its NetworkX id) to its new outgoing connections. Connections that already exist are ignored. key : The jax.random.PRNGKey used for new weight initialization. Optional, keyword-only argument. Defaults to jax.random.PRNGKey(0) . Returns: A NeuralNetwork object with the specified connections added and original parameters retained.","title":"add_connections()"},{"location":"api/plasticity/#connex.remove_connections","text":"Remove connections from the network. Arguments: network : A NeuralNetwork object. connections : The directed edges to remove. Must be a sequence of 2-tuples, or an adjacency dict mapping an existing neuron (by its NetworkX id) to its new outgoing connections. Connections that already exist are ignored. Returns: A NeuralNetwork object with the specified connections removed and original parameters retained.","title":"remove_connections()"},{"location":"api/plasticity/#connex.add_input_neurons","text":"Add input neurons to the network. Note that this function only adds neurons themselves, not any connections associated with the new neurons, effectively adding them as isolated nodes in the graph. Use connex.add_connections after this function has been called to add the desired connections. Arguments: network : The NeuralNetwork to add neurons to new_input_neurons : A sequence of new input neurons (more specifically, their identifiers/names) to add to the network. These must be unique, i.e. cannot already exist in the network. These must also specifically be input neurons. To add hidden or output neurons, use connex.add_hidden_neurons or connex.add_output_neurons . key : The jax.random.PRNGKey used for new parameter initialization. Optional, keyword-only argument. Defaults to jax.random.PRNGKey(0) . Returns: A NeuralNetwork with the new input neurons added and parameters from the original network retained. The new input neurons are added before the existing input neurons. For example, if the previous input neurons were [0, 1] and new input neurons [2, 3] were added, the new input neurons would be [2, 3, 0, 1] .","title":"add_input_neurons()"},{"location":"api/plasticity/#connex.add_hidden_neurons","text":"Add hidden neurons to the network. Note that this function only adds neurons themselves, not any connections associated with the new neurons, effectively adding them as isolated nodes in the graph. Use connex.add_connections after this function has been called to add the desired connections. Arguments: network : The NeuralNetwork to add neurons to new_hidden_neurons : A sequence of new hidden neurons (more specifically, their identifiers/names) to add to the network. These must be unique, i.e. cannot already exist in the These must also specifically be hidden neurons. To add input or output neurons, use connex.add_input_neurons or connex.add_output_neurons . key : The jax.random.PRNGKey used for new parameter initialization. Optional, keyword-only argument. Defaults to jax.random.PRNGKey(0) . Returns: A NeuralNetwork with the new hidden neurons added and parameters from the original network retained.","title":"add_hidden_neurons()"},{"location":"api/plasticity/#connex.add_output_neurons","text":"Add output neurons to the network. Note that this function only adds neurons themselves, not any connections associated with the new neurons, effectively adding them as isolated nodes in the graph. Use connex.add_connections after this function has been called to add any desired connections. Arguments: network : The NeuralNetwork to add neurons to new_output_neurons : A sequence of new output neurons (more specifically, their identifiers/names) to add to the network. These must be unique, i.e. cannot already exist in the network. These must also specifically be output neurons. To add input or output neurons, use connex.add_input_neurons or connex.add_output_neurons . key : The jax.random.PRNGKey used for new parameter initialization. Optional, keyword-only argument. Defaults to jax.random.PRNGKey(0) . Returns: A NeuralNetwork with the new output neurons added and parameters from the original network retained.","title":"add_output_neurons()"},{"location":"api/plasticity/#connex.remove_neurons","text":"Remove neurons and any of their incoming/outgoing connections from the network. Arguments: network : The NeuralNetwork to add neurons to neurons : A sequence of neurons (more specifically, their identifiers/names) to remove from the network. These can be input, hidden, or output neurons. Returns: A NeuralNetwork with the specified neurons removed and parameters from the original network retained.","title":"remove_neurons()"},{"location":"api/plasticity/#connex.set_dropout_p","text":"Set the per-neuron dropout probabilities. Arguments: network : The NeuralNetwork whose dropout probabilities will be modified. dropout_p : Either a float or mapping from neuron ( Any ) to float. If a single float, all hidden neurons will have that dropout probability, and all input and output neurons will have dropout probability 0 by default. If a Mapping , it is assumed that dropout_p maps a neuron to its dropout probability, and all unspecified neurons will retain their current dropout probability. Returns: A copy of the network with dropout probabilities as specified. The original network (including unspecified dropout probabilities) is left unchanged.","title":"set_dropout_p()"},{"location":"api/nn/dense_mlp/","text":"DenseMLP \u00a4 connex.nn.DenseMLP ( NeuralNetwork ) \u00a4 A \"Dense Multi-Layer Perceptron\". Like a standard MLP, but every neuron is connected to every other neuron in all later layers, rather than only the next layer. That is, each layer uses the outputs from all previous layers, not just the most recent one, in a similar manner to DenseNet. Cite Densely Connected Convolutional Networks @article { ba2016layer , author = {Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger} , # noqa: E501 title={Densely Connected Convolutional Networks } , year={2016}, journal={arXiv:1608.06993}, } __init__ ( self , input_size : int , output_size : int , width : int , depth : int , hidden_activation : Callable = < function gelu > , output_transformation : Callable = < function _identity > , dropout_p : Union [ float , Mapping [ Any , float ]] = 0.0 , use_topo_norm : bool = False , use_topo_self_attention : bool = False , use_neuron_self_attention : bool = False , use_adaptive_activations : bool = False , * , key : Optional [ PRNGKey ] = None ) \u00a4 Arguments : input_size : The number of neurons in the input layer. output_size : The number of neurons in the output layer. width : The number of neurons in each hidden layer. depth : The number of hidden layers. hidden_activation : The activation function applied element-wise to the hidden (i.e. non-input, non-output) neurons. It can itself be a trainable equinox Module . output_transformation : The transformation applied group-wise to the output neurons, e.g. jax.nn.softmax . It can itself be a trainable equinox.Module . dropout_p : Dropout probability. If a single float , the same dropout probability will be applied to all hidden neurons. If a Mapping[Any, float] , dropout_p[i] refers to the dropout probability of neuron i . All neurons default to zero unless otherwise specified. Note that this allows dropout to be applied to input and output neurons as well. use_topo_norm : A bool indicating whether to apply a topological batch- version of Layer Norm, Cite Layer Normalization @article { ba2016layer , author = {Jimmy Lei Ba, Jamie Ryan Kriso, Geoffrey E. Hinton} , title = {Layer Normalization} , year = {2016} , journal = {arXiv:1607.06450} , } where the collective inputs of each topological batch are standardized (made to have mean 0 and variance 1), with learnable elementwise-affine parameters gamma and beta . - use_topo_self_attention : A bool indicating whether to apply (single-headed) self-attention to each topological batch's collective inputs. Cite Attention is All You Need @inproceedings { vaswani2017attention , author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\\L}ukasz and Polosukhin, Illia} , booktitle = {Advances in Neural Information Processing Systems} , publisher = {Curran Associates, Inc.} , title = {Attention is All You Need} , volume = {30} , year = {2017} } use_neuron_self_attention : A bool indicating whether to apply neuron-wise self-attention, where each neuron applies (single-headed) self-attention to its inputs. If both _use_neuron_self_attention and use_neuron_norm are True , normalization is applied before self-attention. Warning Neuron-level self-attention will use significantly more memory than than topo-level self-attention. use_adaptive_activations : A bool indicating whether to use neuron-wise adaptive activations, where all hidden activations transform as \u03c3(x) -> a * \u03c3(b * x) , where a , b are trainable scalar parameters unique to each neuron. Cite Locally adaptive activation functions with slope recovery term for deep and physics-informed neural networks # noqa: E501 @article { Jagtap_2020 , author = {Ameya D. Jagtap, Kenji Kawaguchi, George Em Karniadakis} , title = {Locally adaptive activation functions with slope recovery term for deep and physics-informed neural networks} , year = {2020} , publisher = {The Royal Society} , journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences} , } key : The PRNGKey used to initialize parameters. Optional, keyword-only argument. Defaults to jax.random.PRNGKey(0) .","title":"DenseMLP"},{"location":"api/nn/dense_mlp/#densemlp","text":"","title":"DenseMLP"},{"location":"api/nn/dense_mlp/#connex.nn.DenseMLP","text":"A \"Dense Multi-Layer Perceptron\". Like a standard MLP, but every neuron is connected to every other neuron in all later layers, rather than only the next layer. That is, each layer uses the outputs from all previous layers, not just the most recent one, in a similar manner to DenseNet. Cite Densely Connected Convolutional Networks @article { ba2016layer , author = {Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger} , # noqa: E501 title={Densely Connected Convolutional Networks } , year={2016}, journal={arXiv:1608.06993}, }","title":"DenseMLP"},{"location":"api/nn/mlp/","text":"MLP \u00a4 connex.nn.MLP ( NeuralNetwork ) \u00a4 A standard Multi-Layer Perceptron with constant layer width. __init__ ( self , input_size : int , output_size : int , width : int , depth : int , hidden_activation : Callable = < function gelu > , output_transformation : Callable = < function _identity > , dropout_p : Union [ float , Mapping [ Any , float ]] = 0.0 , use_topo_norm : bool = False , use_topo_self_attention : bool = False , use_neuron_self_attention : bool = False , use_adaptive_activations : bool = False , * , key : Optional [ PRNGKey ] = None ) \u00a4 Arguments : input_size : The number of neurons in the input layer. output_size : The number of neurons in the output layer. width : The number of neurons in each hidden layer. depth : The number of hidden layers. hidden_activation : The activation function applied element-wise to the hidden (i.e. non-input, non-output) neurons. It can itself be a trainable equinox Module . output_transformation : The transformation applied group-wise to the output neurons, e.g. jax.nn.softmax . It can itself be a trainable equinox.Module . dropout_p : Dropout probability. If a single float , the same dropout probability will be applied to all hidden neurons. If a Mapping[Any, float] , dropout_p[i] refers to the dropout probability of neuron i . All neurons default to zero unless otherwise specified. Note that this allows dropout to be applied to input and output neurons as well. use_topo_norm : A bool indicating whether to apply a topological batch- version of Layer Norm, Cite Layer Normalization @article { ba2016layer , author = {Jimmy Lei Ba, Jamie Ryan Kriso, Geoffrey E. Hinton} , title = {Layer Normalization} , year = {2016} , journal = {arXiv:1607.06450} , } where the collective inputs of each topological batch are standardized (made to have mean 0 and variance 1), with learnable elementwise-affine parameters gamma and beta . - use_topo_self_attention : A bool indicating whether to apply (single-headed) self-attention to each topological batch's collective inputs. Cite Attention is All You Need @inproceedings { vaswani2017attention , author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\\L}ukasz and Polosukhin, Illia} , booktitle = {Advances in Neural Information Processing Systems} , publisher = {Curran Associates, Inc.} , title = {Attention is All You Need} , volume = {30} , year = {2017} } use_neuron_self_attention : A bool indicating whether to apply neuron-wise self-attention, where each neuron applies (single-headed) self-attention to its inputs. If both _use_neuron_self_attention and use_neuron_norm are True , normalization is applied before self-attention. Warning Neuron-level self-attention will use significantly more memory than than topo-level self-attention. use_adaptive_activations : A bool indicating whether to use neuron-wise adaptive activations, where all hidden activations transform as \u03c3(x) -> a * \u03c3(b * x) , where a , b are trainable scalar parameters unique to each neuron. Cite Locally adaptive activation functions with slope recovery term for deep and physics-informed neural networks # noqa: E501 @article { Jagtap_2020 , author = {Ameya D. Jagtap, Kenji Kawaguchi, George Em Karniadakis} , title = {Locally adaptive activation functions with slope recovery term for deep and physics-informed neural networks} , year = {2020} , publisher = {The Royal Society} , journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences} , } key : The PRNGKey used to initialize parameters. Optional, keyword-only argument. Defaults to jax.random.PRNGKey(0) .","title":"MLP"},{"location":"api/nn/mlp/#mlp","text":"","title":"MLP"},{"location":"api/nn/mlp/#connex.nn.MLP","text":"A standard Multi-Layer Perceptron with constant layer width.","title":"MLP"}]}